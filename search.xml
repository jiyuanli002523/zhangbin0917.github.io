<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[论文阅读 - SSD: Single Shot MultiBox Detector]]></title>
    <url>%2F2018%2F09%2F29%2FSSD-Single-Shot-MultiBox-Detector%2F</url>
    <content type="text"><![CDATA[作者主页：https://www.cs.unc.edu/~wliu/arxiv: http://arxiv.org/abs/1512.02325paper: http://www.cs.unc.edu/~wliu/papers/ssd.pdfslides: http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdfcode: https://github.com/weiliu89/caffe/tree/ssd Network StructureSSD全部使用了卷积层，并且在多个特征图使用3×3的卷积预测bounding box及其种类。 Base NetworkBase Network为VGG16，与原始的VGG16的不同为： 将fc6和fc7由全连接层转为卷积层，fc6为dilation为6的3×3卷积，fc7为1×1的卷积 将pool5的2×2的size，stride为2转为3×3的size，stride为1 Multi-scale feature maps for detectionSSD中一共使用了6个不同尺度的特征图，分别为：Conv4_3，fc7，Conv8_2，Conv9_2，Conv10_2，Conv11_2，其中Conv4_3，fc7为Base Network中的。Conv4_3特征图大小为38×38，fc7特征图大小为19×19。在Base Network后又加了额外的的结构，特征图大小持续减小到1×1： Conv8_1: 1x1x256 Conv8_2: 3x3x512-s2 feature map size: 10×10 Conv9_1: 1x1x128 Conv9_2: 3x3x256-s2 feature map size: 5×5 Conv10_1: 1x1x128 Conv10_2: 3x3x256-s1 feature map size: 3×3 Conv11_1: 1x1x128 Conv11_2: 3x3x256-s1 feature map size: 1×1 Convolutional predictors for detection对于这6个特征图，每个位置会生成多个大小和比例的default box，在每个特征图上使用3×3的卷积预测每个位置的每个bounding box的score for a category和shape offset relative to the default box coordinates。例如对Conv4_3特征图来说，Conv4_3 shape为(38,38,512), 每个位置生成4个default box，假设有classes个类别，使用4classes个卷积核为3×3×512的卷积预测每个bounding box对每个类别得分，由于预测一个bounding box需要4个参数，所以使用4*4个卷积核为3×3×512的卷积预测ground truth bounding box对每个default bounding box的偏移。6个特征图一共可以生成8732个default box，Conv4_3，Conv10_2，Conv11_2采用了4种大小的box，Conv7，Conv8_2，Conv9_2采用了6种大小的box： Conv4_3: 38 × 38 × 4 = 5776 Conv7: 19 × 19 × 6 = 2166 Conv8_2: 10 × 10 × 6 = 600 Conv9_2: 5 × 5 × 6 = 150 Conv10_2: 3 × 3 × 4 = 36 Conv11_2: 4 Total: 5776+ 2166 + 600 + 150 + 36 + 4 = 8732 训练时需要解决的问题Matching strategy训练时需要确定哪个default box和ground truth对应起来。首先将与ground truth的IoU最大的default box作为正样本，这样每个ground truth对应一个正样本，然后将default box与ground truth的IoU大于0.5的作为正样本，这样每个ground truth至少对应一个正样本。 Training objective损失函数包括两部分： localization loss (loc)和confidence loss (conf)，权重项α通过交叉验证设为1其中localization loss只针对正样本而言，为 predicted box (l) 和 ground truth box (g)参数的Smooth L1 loss，具体做法为回归ground truth bounding box和default box关于中心x，y和宽w，高h的相对偏移：confidence loss为softmax loss，对正样本来说需要其对应类别的概率最大，对负样本来说需要其检测为背景的概率最大： Choosing scales and aspect ratios for default boxes假设有m个特征图，每个特征图的default box的scale按以下公式计算：有了scale就可以计算box的w和h，作者一共采用了6中长宽和大小不同的box： 比例α为{1,2,3,1/2,1/3}，w=s*sqrt(α)，h=s/sqrt(α) 正方形w=h=s=sqrt(sk*sk+1) Hard negative mining由于大多数default box是负样本，如果使用全部的负样本训练会使训练不稳定，因此对负样本对confidence loss进行降序排列，选择最高前3倍的正样本数量的进行训练，控制负样本和正样本比例为3:1。 Data augmentation Use the entire original input image. Sample a patch so that the minimum jaccard overlap with the objects is 0.1, 0.3, 0.5, 0.7, or 0.9. Randomly sample a patch. resized to fixed size horizontally flipped with probability of 0.5 applying some photo-metric distortions Non-Maximum SuppressionSSD sorts the predictions by the confidence scores. Start from the top confidence prediction, SSD evaluates whether any previously predicted boundary boxes have an IoU higher than 0.45 with the current prediction for the same class. If found, the current prediction will be ignored. At most, we keep the top 200 predictions per image. Experimental ResultsPASCAL VOC2007 can detect various object categories with high quality has less localization error, indicating that SSD can localize objects better because it directly learns to regress the object shape and classify object categories instead of using two decoupled steps more confusions with similar object categories (especially for animals) very sensitive to the bounding box size has much worse performance on smaller objects than bigger objects performs really well on large objects very robust to different object aspect ratios because use default boxes of various aspect ratios per feature map location Model analysis Data augmentation is crucial More default box shapes is better Atrous is faster Multiple output layers at different resolutions is better PASCAL VOC2012 COCO Data Augmentation for Small Object Accuracy Inference time 其他细节 Prior Box min size和max size的生成方式：见源码中ssd_pascal.py文件: 1234567891011121314151617181920212223# parameters for generating priors.# minimum dimension of input imagemin_dim = 300# conv4_3 ==&gt; 38 x 38# fc7 ==&gt; 19 x 19# conv6_2 ==&gt; 10 x 10# conv7_2 ==&gt; 5 x 5# conv8_2 ==&gt; 3 x 3# conv9_2 ==&gt; 1 x 1mbox_source_layers = ['conv4_3', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2']# in percent %min_ratio = 20max_ratio = 90step = int(math.floor((max_ratio - min_ratio) / (len(mbox_source_layers) - 2)))min_sizes = []max_sizes = []for ratio in xrange(min_ratio, max_ratio + 1, step): min_sizes.append(min_dim * ratio / 100.) max_sizes.append(min_dim * (ratio + step) / 100.)min_sizes = [min_dim * 10 / 100.] + min_sizesmax_sizes = [min_dim * 20 / 100.] + max_sizessteps = [8, 16, 32, 64, 100, 300]aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]] L2 Normalization on conv4_3作者在conv4_3后加入了L2 Normalization，见protxt： 1234567891011121314layer &#123; name: "conv4_3_norm" type: "Normalize" bottom: "conv4_3" top: "conv4_3_norm" norm_param &#123; across_spatial: false scale_filler &#123; type: "constant" value: 20.0 &#125; channel_shared: false &#125;&#125; 为什么要加入L2 Normalization?参见作者的回答:That was discovered in my other paper (ParseNet) that conv4_3 has different scale from other layers. That is why I add L2 normalization for conv4_3 only.在这个issue下ujsyehao贴出了不同层的权重的分布图实际上就是对conv4_3的特征图在通道维度上做归一化，再乘以系数20. Variance的作用PriorBox layers的参数Variance的作用：the major goal of including the variance is to scale the gradient参见作者的回答:https://github.com/weiliu89/caffe/issues/75https://github.com/weiliu89/caffe/issues/155https://github.com/weiliu89/caffe/issues/629]]></content>
      <categories>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - NeuroIoU: Learning a Surrogate Loss for Semantic Segmentation(BMVC 2018)]]></title>
    <url>%2F2018%2F09%2F20%2FNeuroIoU-Learning-a-Surrogate-Loss-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[语义分割问题中评价指标往往为IoU，由于其不可微，使用逐像素的cross entropy并不能直接优化这个指标，而导致网络性能结果较差。因此作者提出在常规语义分割网络后用一个网络来拟合IoU，进而可以进行求导和反向传播优化IoU。 在语义分割任务中一般使用cross entropy loss，由于IoU在类别不平衡时可以更好的评价网络性能，所以评价指标往往为IoU。如下图，(c)和(d)分类精度相同，但很明显(d)的分割结果要比(c)好。由于IoU不可微，所以其不能作为损失函数。因此作者提出在现有的分割网络后加一个全连接网络来拟合IoU Loss作为Surrogate IoU Loss，作者称这个Surrogate IoU Loss为NeuroIoU loss。 Surrogate IoU LossIoU的定义为： IoU=\frac{TP}{TP+FP+FN}因此IoU loss定义为： L_{IoU}=1-IoU=1-\frac{TP}{TP+FP+FN}=\frac{FP+FN}{TP+FP+FN}由于IoU loss不可微，所以使用一个代理网络来估计IoU loss Defining the Surrogate IoU-Loss Network首先定义Surrogate IoU-Loss Network的输入,设x为预训练的语义分割网络的输出,设$TP_{pr}$,$FP_{pr}$,$FN_{pr}$的概率个数为分别属于TP，FP，FN的像素个数： TP_{pr}=\sum_{x_{i}}{P(x_{i})},where\, x_{i}\in{TP}FP_{pr}=\sum_{x_{i}}{P(x_{i})},where\, x_{i}\in{FP}FN_{pr}=\sum_{x_{i}}{P(x_{i})},where\, x_{i}\in{FN}Surrogate IoU-Loss Network的输出为$L_{IoU}$。作者设计这个网络为4层100个神经元，将这个网络叫做NeuroIoU,损失为网络的输出和$L_{IoU}$的 Mean-Squared Error loss: E(w)=\sum_{i}{[NeuroIoU(TP_{pr_{i}},FP_{pr_{i}},FN_{pr_{i}})-L_{IoU_{i}}]^2}但是不同的分割结果可能会有相同的IoU，这会影响估计IoU的性能，为了使估计IoU更鲁棒将$\frac{TP_{pr}}{\left | TP \right |}$,$\frac{FP_{pr}}{\left | FP \right |}$,$\frac{FN_{pr}}{\left | FN \right |}$也作为网络的输入.实验证明输入6个时，更加鲁棒 Training Semantic Segmentation Models with NeuroIoU Loss用训练好的NeuroIoU再去fine-tune语义分割网络 Approximation using Prediction Loss也可以使用hinge loss来估计。分别计算属于TP，FP，FN的像素的hinge loss的和。 Experiments]]></content>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Loss Fuction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种新的regularization方法]]></title>
    <url>%2F2018%2F09%2F18%2F%E5%87%A0%E7%A7%8D%E6%96%B0%E7%9A%84regularization%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[几种新的regularization方法： Shake-Shake Cutout mixup pairing samples ShakeDrop AutoAugment Shake-Shakecode: https://github.com/xgastaldi/shake-shakeGastaldi X. Shake-shake regularization[J]. arXiv preprint arXiv:1705.07485, 2017.The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination.Cutoutcode: https://github.com/uoguelph-mlrg/CutoutDeVries T, Taylor G W. Improved regularization of convolutional neural networks with cutout[J]. arXiv preprint arXiv:1708.04552, 2017.Randomly masking out square regions of input during trainingmixupZhang H, Cisse M, Dauphin Y N, et al. mixup: Beyond empirical risk minimization[J]. arXiv preprint arXiv:1710.09412, 2017.pairing samplesInoue H. Data augmentation by pairing samples for images classification[J]. arXiv preprint arXiv:1801.02929, 2018.ShakeDropcode: https://github.com/imenurok/ShakeDropTo realize a similar regularization to Shake-Shake on 1-branch network architecturesYamada Y, Iwamura M, Kise K. ShakeDrop regularization[J]. arXiv preprint arXiv:1802.02375, 2018.AutoAugmentcode: https://github.com/DeepVoltaire/AutoAugmentCubuk E D, Zoph B, Mane D, et al. AutoAugment: Learning Augmentation Policies from Data[J]. arXiv preprint arXiv:1805.09501, 2018.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Semantic Segmentation]]></title>
    <url>%2F2018%2F09%2F18%2FSemantic-Segmentation%2F</url>
    <content type="text"><![CDATA[network mAP(VOC12) mAP(VOC12 with COCO) mAP(Pascal Context) mAP(CamVid) mAP(Cityscapes) mAP(ADE20K) Published In FCN-8s 62.2 37.8 65.3 CVPR 2015 DeepLab 71.6 ICLR 2015 CRF-RNN 72.0 74.7 39.3 ICCV 2015 DeconvNet 72.5 ICCV 2015 DPN 74.1 77.5 ICCV 2015 SegNet 50.2 Dilation8 75.3 Deeplab v2 79.7 45.7 70.4 PAMI FRRN B 71.8 CVPR 2017 G-FRNet 79.3 68.0 CVPR 2017 GCN 82.2 76.9 CVPR 2017 SegModel 82.5 79.2 CVPR 2017 RefineNet 83.4 47.3 73.6 40.7 CVPR 2017 PSPNet 82.6 85.4 80.2 CVPR 2017 DIS 86.8 ICCV 2017 SAC-multiple 78.1 44.3 ICCV 2017 DeepLabv3 85.7 81.3 arxiv 1706.05587 DUC-HDC 80.1 WACV2018 DDSC 81.2 47.8 70.9 CVPR 2018 EncNet 82.9 85.9 51.7 44.65 CVPR 2018 DFN 82.7 86.2 80.3 CVPR 2018 DenseASPP 80.6 CVPR 2018 UperNet 42.66 ECCV 2018 PSANet 85.7 80.1 43.77 ECCV 2018 DeepLabv3+ 87.8 82.1 ECCV 2018 ExFuse 87.9 ECCV 2018 OCNet 81.2(81.7) 45.08(45.45) arxiv 1809.00916 DAN 52.6 78.2 CVPR 2019 DPC 87.9 82.7 NIPS 2018 CCNet 81.4 45.22 arxiv 1811.11721 TKCN 83.2 79.5 ICME 2019 DUpsampling 85.3 88.1 52.5 CVPR 2019 FastFCN 53.1 44.34 Semantic Segmentation论文整理 dataset [NYU2] [ECCV2012] Indoor segmentation and support inference from rgbd images [SUN RGB-D] [CVPR2015] SUN RGB-D: A RGB-D scene understanding benchmark suite shuran [Matterport3D] Matterport3D: Learning from RGB-D Data in Indoor Environments [Paper] 2D Semantic Segmentation [FCN] Fully Convolutional Networks for Semantic Segmentation [Paper1] [Paper2] [Slides1] [Slides2] [DeepLab v1] Semantic Image Segmentation With Deep Convolutional Nets and Fully Connected CRFs[Code-Caffe] [Paper] [CRF as RNN] Conditional Random Fields as Recurrent Neural Networks [Project] [Demo] [Paper] [DeconvNet] Learning Deconvolution Network for Semantic Segmentation [Project] [Paper] [Slides] [U-Net] U-Net: Convolutional Networks for Biomedical Image Segmentation [Project] [Paper] [SegNet] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation [Project] [Paper] [Tutorial1] [Tutorial2] Multi-scale context aggregation by dilated convolutions [Paper] [DeepLab v2] DeepLab v2:Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs** [Project] [Code-Caffe] [Paper] [RefineNet] [CVPR2017] RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation [Code-MatConvNet] [Paper] [IFCN] Improving Fully Convolution Network for Semantic Segmentation [Paper] [FC-DenseNet] [CVPRW2017] The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation [Code-Theano] [Code-Keras1] [Code-Keras2] [Paper] [PSPNet] [CVPR2017] Pyramid Scene Parsing Network [Project] [Code-Caffe] [Paper] [Slides] [FusionNet] FusionNet: A deep fully residual convolutional neural network for image segmentation in connectomics [Code-PyTorch] [Paper] [PixelNet] PixelNet: Representation of the pixels, by the pixels, and for the pixels [Project] [Code-Caffe] [Paper] [DUC-HDC] [WACV 2018]Understanding Convolution for Semantic Segmentation [Model-Mxnet] [Paper] [Code] [GCN] [CVPR2017] Large Kernel Matters - Improve Semantic Segmentation by Global Convolutional Network [Paper] [CVPR 2017] Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade-2017 [Paper] Pixel Deconvolutional Networks-2017 [Code-Tensorflow] [Paper] [DRN] [CVPR 2017] Dilated Residual Networks [Paper] [Code] [Deeplab v3] Deeplab v3: Rethinking Atrous Convolution for Semantic Image Segmentation [Paper] [LinkNet] LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation [Paper] [SDN] Stacked Deconvolutional Network for Semantic Segmentation [Paper] Learning to Segment Every Thing [Paper] Panoptic Segmentation [Paper] [DeepLabv3+] [ECCV 2018] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation [Paper] [Code] [EncNet] [CVPR 2018] Context Encoding for Semantic Segmentation [Paper] [Code] (Leverages global context to increase accuracy by adding a channel attention module, which triggers attention on certain feature maps based on a newly designed loss function. The loss is based on a network branch which predicts which classes are present in the image (i.e higher level global context)) [ECCV 2018] Adaptive Affinity Fields for Semantic Segmentation [Project] [Paper] [Code] [EXFuse] [ECCV 2018] ExFuse: Enhancing Feature Fusion for Semantic Segmentation [Paper] (Uses deep supervision and explicitly combines the multi-scale features from the feature extraction frontend before processing, in order to ensure multi-scale information is processed together at all levels) Vortex Pooling: Improving Context Representation in Semantic Segmentation [Paper] [DFN] [CVPR 2018] Learning a Discriminative Feature Network for Semantic Segmentation [Paper] (Uses deep supervision and attempts to process the smooth and edge portions of the segments separately) Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation [Paper] [BMVC 2018] Pyramid Attention Network for Semantic Segmentation [Paper] [G-FRNet] [CVPR 2017] Gated Feedback Refinement Network for Coarse-to-Fine Dense Semantic Image Labeling [Paper] [code] [CVPR 2018] Context Contrasted Feature and Gated Multi-Scale Aggregation for Scene Segmentation [Paper] [DenseASPP] [CVPR 2018] DenseASPP for Semantic Segmentation in Street Scenes [Paper] [code] (Combines dense connections with atrous convolutions) [CVPR 2018] Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation [Paper] (Use dense connections in the decoding stage for higher accuracy (previously only done during feature extraction / encoding)) Smoothed Dilated Convolutions for Improved Dense Prediction [Paper] [PSANet] [ECCV 2018] PSANet: Point-wise Spatial Attention Network for Scene Parsing [Paper] [project] [code] [slide] (Attention Mechanism) [OCNet] OCNet: Object Context Network for Scene Parsing [Paper] [code] (Attention Mechanism) [DAN] [CVPR 2019] Dual Attention Network for Scene Segmentation [Paper] [code] (Attention Mechanism) [CCNet] CCNet: Criss-Cross Attention for Semantic Segmentation [Paper] [code] (Attention Mechanism) [TKCN] Tree-structured Kronecker Convolutional Networks for Semantic Segmentation [Paper] [code] Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation[Paper] Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation [Paper] [CVPR 2019] Structured Knowledge Distillation for Semantic Segmentation [Paper] [CVPR 2019] Knowledge Adaptation for Efficient Semantic Segmentation [Paper] [CVPR 2019] A Cross-Season Correspondence Dataset for Robust Semantic Segmentation [Paper] Efficient Smoothing of Dilated Convolutions for Image Segmentation [Paper] [Code] FastFCN：Rethinking Dilated Convolution in the Backbone for Semantic Segmentation [Paper] [Code] Real-Time Semantic Segmentation [ENet] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation-2016 [Paper] [ICNet] [ECCV 2018] ICNet for Real-Time Semantic Segmentation on High-Resolution Images [Project] [Code] [Paper] [Video] (Uses deep supervision and runs the input image at different scales, each scale through their own subnetwork and progressively combining the results) [RTSeg] RTSeg: Real-time Semantic Segmentation Comparative Study [Paper] [ShuffleSeg] ShuffleSeg: Real-time Semantic Segmentation Network [Paper] [ESPNet] [ECCV 2018] ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation [Paper] [ContextNet] [BMVC 2018] ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time [Paper] Guided Upsampling Network for Real-Time Semantic Segmentation [Project] [Paper] [BiSeNet] [ECCV 2018] BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation [Paper] (Has 2 branches: one is deep for getting semantic information, while the other does very little / minor processing on the input image as to preserve the low-level pixel information) Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation [Paper] [BMVC 2018] Light-Weight RefineNet for Real-Time Semantic Segmentation [Paper] [code] CGNet: A Light-weight Context Guided Network for Semantic Segmentation [Paper] [Code] ShelfNet for Real-time Semantic Segmentation [Paper] ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network [Paper][Code] Real time backbone for semantic segmentation [Paper] DSNet for Real-Time Driving Scene Semantic Segmentation [Paper] In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images [Paper] Residual Pyramid Learning for Single-Shot Semantic Segmentation [Paper] Loss Fuction The Lovász Hinge: A Novel Convex Surrogate for Submodular Losses [arxiv] [project] [CVPR 2017 ] Loss Max-Pooling for Semantic Image Segmentation [Paper] [CVPR 2018] The Lovász-Softmax loss：A tractable surrogate for the optimization of the intersection-over-union measure in neural networks [Project] [Paper] [Code] Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations [Paper] IoU is not submodular [arxiv] Yes, IoU loss is submodular - as a function of the mispredictions [arxiv] [BMVC 2018] NeuroIoU: Learning a Surrogate Loss for Semantic Segmentation [Paper] [code] Review A Survey of Semantic Segmentation [arxiv] A Review on Deep Learning Techniques Applied to Semantic Segmentation [arxiv] Recent progress in semantic image segmentation [arxiv]]]></content>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design(ECCV2018 Megvii Inc)]]></title>
    <url>%2F2018%2F09%2F15%2FShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design%2F</url>
    <content type="text"><![CDATA[提出针对移动端深度学习的第二代卷积神经网络 ShuffleNet V2。研究者指出过去在网络架构设计上仅注重间接指标 FLOPs 的不足，并提出两个基本原则和四项准则来指导网络架构设计，最终得到了无论在速度还是精度上都超越先前最佳网络（例如 ShuffleNet V1、MobileNet 等）的 ShuffleNet V2。在综合实验评估中，ShuffleNet V2 也在速度和精度之间实现了最佳权衡。 paper: https://arxiv.org/abs/1807.11164http://openaccess.thecvf.com/content_ECCV_2018/html/Ningning_Light-weight_CNN_Architecture_ECCV_2018_paper.html第三方code： https://github.com/miaow1988/ShuffleNet_V2_pytorch_caffe https://github.com/Randl/ShuffleNetV2-pytorch https://github.com/ericsun99/Shufflenet-v2-Pytorch Introduction计算复杂度是深度卷积网络另一个重要的考虑因素。现实世界的任务通常是当目标平台（如硬件）和应用场景（如自动驾驶需要低延迟）既定时，在有限算力之下实现最优的精度。这催生出一系列针对轻量级架构设计和速度-精度更好权衡的研究，包括 Xception、MobileNet、MobileNet V2、ShuﬄeNet V1和 CondenseNet等等。在这些研究中，group convolution和depthwise convolution至关重要。度量计算复杂度的常用指标是浮点运算数，即 FLOPs。然而，FLOPs 是一种间接指标。它只是直接指标（如速度或延迟）的一种近似形式，通常无法与直接指标划等号。先前研究如MobileNet V2要远快于 NASNET-A，但是两者 FLOPs 相当。图 1 (c) (d) 进一步解释了这一现象，它表明 FLOPs 近似的网络也会有不同的速度。所以，将 FLOPs 作为衡量计算复杂度的唯一标准是不够的，这样会导致次优设计。 间接指标 (FLOPs) 和直接指标（速度）之间存在差异的原因可以归结为两点。首先，对速度有较大影响的几个重要因素对 FLOPs 不产生太大作用。其中一个因素是内存访问成本 (MAC)。在某些操作（如组卷积）中，MAC 占运行时间的很大一部分。对于像 GPU 这样具备强大计算能力的设备而言，这就是瓶颈。在网络架构设计过程中，内存访问成本不能被简单忽视。另一个因素是并行度。当 FLOPs 相同时，高并行度的模型可能比低并行度的模型快得多。其次，FLOPs 相同的运算可能有着不同的运行时间，这取决于平台。 据此，研究者提出了高效网络架构设计应该考虑的两个基本原则：第一，应该用直接指标（例如速度）替换间接指标（例如 FLOPs）；第二，这些指标应该在目标平台上进行评估。在这项研究中，作者遵循这两个原则，并提出了一种更加高效的网络架构。 Practical Guidelines for Efficient Network Design研究者分析了两个移动端当前最佳网络 ShuffleNet V1 和 MobileNet V2 的运行时性能，发现它们的表现代表了当前的研究趋势。它们的核心组件为组卷积和深度卷积，这也是其它当前最佳架构的关键组件，例如 ResNet、Xception、MobileNet 和 CondenseNet 等。 研究者从不同层面做了运行时（速度方面）分析，并提出了设计高效网络架构需要遵循的准则： G1. 相同的通道宽度可最小化内存访问成本（MAC）； G2. 过度的组卷积会增加 MAC； G3. 网络碎片化（例如 GoogLeNet 的多路径结构）会降低并行度； G4. 元素级运算不可忽视。 结论和讨论：基于上述准则和实证研究，本文总结出一个高效的网络架构应该：（1）使用「平衡」的卷积（相同的通道宽度）；（2）考虑使用组卷积的成本；（3）降低碎片化程度；（4）减少元素级运算。这些所需特性依赖于平台特征（例如内存控制和代码优化），且超越了理论化的 FLOPs。它们都应该在实际的网络设计中被考虑到。 近期，轻量级神经网络架构上的研究进展主要基于 FLOPs 间接指标，并且没有考虑上述四个准则。例如，ShuffleNet V1严重依赖组卷积（违反 G2）和瓶颈形态的构造块（违反 G1）。MobileNet V2使用一种倒置的瓶颈结构，违反了 G1。它在「厚」特征图上使用了深度卷积和 ReLU 激活函数，违反了 G4。自动生成的网络的碎片化程度很高，违反了 G3。 ShuffleNet V2根据 ShuﬄeNet V1，轻量级网络的主要挑战是在给定计算预算（FLOPs）时，只能获得有限数量的特征通道。为了在不显著增加 FLOPs 情况下增加通道数量，ShuﬄeNet V1 采用了两种技术：逐点组卷积核和类瓶颈（bottleneck-like）结构；然后引入「channel shuﬄe」操作，令不同组的通道之间能够进行信息交流，提高精度。其构建模块如图 3(a)(b) 所示。逐点组卷积和瓶颈结构都增加了 MAC(G1 和 G2)。这个成本不可忽视，特别是对于轻量级模型。另外，使用太多分组也违背了 G3。shortcut connection中的元素级「加法」操作也不可取 (G4)。因此，为了实现较高的模型容量和效率，关键问题是如何保持大量且同样宽的通道，既没有密集卷积也没有太多的分组。 通道分割和 ShuﬄeNet V2为此，本文引入一个简单的操作——通道分割（channel split）。如图 3(c) 所示。在每个单元的开始，c 特征通道的输入被分为两支，分别带有 c−c’和 c’个通道。按照准则 G3，一个分支仍然保持不变。另一个分支由三个卷积组成，为满足 G1，令输入和输出通道相同。与 ShuffleNet V1 不同的是，两个 1×1 卷积不再是组卷积。这部分是为了遵循 G2，部分是因为分割操作已经产生了两个组。卷积之后，把两个分支拼接起来，从而通道数量保持不变 (G1)。然后进行与 ShuffleNet V1 相同的「Channel Shuﬄe」操作来保证两个分支间能进行信息交流。「Shuffle」之后，下一个单元开始运算。注意，ShuﬄeNet V1中的「加法」操作不再存在。像 ReLU 和深度卷积这样的操作只存在一个分支中。另外，三个连续的操作「拼接」、「Channel Shuﬄe」和「通道分割」合并成一个操作。根据 G4，这些变化是有利的。对于空间下采样，该单元经过稍微修改，详见图 3(d)。通道分割运算被移除。因此，输出通道数量翻了一倍。提出的构造块 (c)(d)，以及由此而得的网络，被称之为 ShuﬄeNet V2。基于上述分析，本文得出结论：由于对上述四个准则的遵循，该架构设计异常高效。上述构建模块被重复堆叠以构建整个网络。为简单起见，令 c’ = c/2。整个网络结构类似于 ShufﬂeNet V1（见表 5）。二者之间只有一个区别：前者在全局平均池化层之前添加了一个额外的 1×1 卷积层来混合特征，ShuﬄeNet V1 中没有该层。与 ShuﬄeNet V1 类似，每个构建块中的通道数量可以扩展以生成不同复杂度的网络，标记为 0.5×、1×等。 Experiment]]></content>
      <categories>
        <category>Image classification</category>
      </categories>
      <tags>
        <tag>Image classification</tag>
        <tag>light-weight architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - PSANet: Point-wise Spatial Attention Network for Scene Parsing(ECCV2018 CUHK)]]></title>
    <url>%2F2018%2F09%2F15%2FPSANet-Point-wise-Spatial-Attention-Network-for-Scene-Parsing%2F</url>
    <content type="text"><![CDATA[这篇论文认为再CNN中信息流被约束在局部区域中，进而限制了复杂场景的理解。因此提出了 point-wise spatial attention network (PSANet)来缓和这种局部约束。通过自适应地attention map将特征图地每个位置联系起来促进信息传递。所提出的网络在ADE20K，PASCAL VOC 2012,Cityscapes证明了其有效性。 作者主页：https://hszhao.github.io/project: https://hszhao.github.io/projects/psanet/paper: http://openaccess.thecvf.com/content_ECCV_2018/html/Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper.htmlcode: https://github.com/hszhao/PSANetslide: https://docs.google.com/presentation/d/1_brKNBtv8nVu_jOwFRGwVkEPAq8B8hEngBSQuZCWaZA/edit?usp=sharing Network StructureOverview PSA module Experimental EvaluationADE20K PASCAL VOC 2012Cityscapes]]></content>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Spatial Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - Dual Attention Network for Scene Segmentation(AAAI 2019 中科院自动化所)]]></title>
    <url>%2F2018%2F09%2F13%2FDual-Attention-Network-for-Scene-Segmentation%2F</url>
    <content type="text"><![CDATA[这篇论文通过基于Self Attention mechanism来捕获上下文依赖。提出了Dual Attention Networks (DANet)来自适应地整合局部特征和全局依赖。具体做法是，在一贯的dilated FCN中加入两种类型地attention module。其中position attention module选择性地通过所有位置的加权求和聚集每个位置的特征。channel attention module通过所有channle的feature map中的特征选择性地强调某个特征图。最后将两种attention module的output 求和得到最后的特征表达。所提出的DANet在三个数据集Cityscapes， PASCAL Context和COCO Stuff上实现了state-of-the-art的结果。 paper: https://arxiv.org/abs/1809.02983code: https://github.com/junfu1115/DANet 主要贡献： 提出了Dual Attention Networks (DANet)在spatial和channle维度来捕获全局特征依赖 提出position attention module去学习空间特征的相关性，提出channel attention module去建模channle的相关性 在三个数据集Cityscapes， PASCAL Context和COCO Stuff上实现了state-of-the-art的结果 Overview基础网络为dilated ResNet(与DeepLab相同)，最后得到的feature map大小为输入图像的1/8。之后是两个并行的attention module分别捕获spatial和channel的依赖性，最后整合两个attention module的输出得到更好的特征表达。 Position Attention Module如图所示，在Position Attention Module中，特征图A(C×H×W)首先分别通过3个卷积层得到3个特征图B，C，D，然后reshape为C×N，其中N=H×W，之后将reshape后的B的转置与reshape后的C相乘再通过softmax得到spatial attention map S(N×N)，接着把S与D做乘积再乘以尺度系数α再reshape为原来形状，最后与A相加得到最后的输出E。其中α初始化为0，并逐渐的学习分配到更大的权重。可以看出E的每个位置的值是原始特征每个位置的加权求和得到的。 Channel Attention Module同样的，在Channel Attention Module中，分别对A做reshape和reshape与transpose，将得到的两个特征图相乘再通过softmax得到channel attention map X(C×C),接着把X与A做乘积再乘以尺度系数β再reshape为原来形状，最后与A相加得到最后的输出E。其中β初始化为0，并逐渐的学习分配到更大的权重。 Attention Module Embedding with Networks两个attention module的输出先求和再做一次卷积得到最后的预测特征图。 ExperimentsResults on Cityscapes DatasetAblation Study for Attention Modules Ablation Study for Improvement Strategies Visualization of Attention Module Comparing with State-of-the-art Results on PASCAL Context Dataset Results on COCO Stuff Dataset]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Self Attention Mechanism</tag>
        <tag>position attention</tag>
        <tag>channle attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - OCNet: Object Context Network for Scene Parsing(Microsoft Research)]]></title>
    <url>%2F2018%2F09%2F13%2FOCNet-Object-Context-Network-for-Scene-Parsing%2F</url>
    <content type="text"><![CDATA[不同于之前图像级的context的方法，这篇论文提出逐像素的object context，object context由像素P所对应的类别的物体组成。由于测试时不知道标签信息，所以用Self Attention方法通过学习逐像素的相似度图估计objects。在此基础上有进一步提出了Pyramid Object Context和Atrous Spatial Pyramid Object Context来捕获多尺度的上下文。在object context module基础上提出了OCNet，OCNet在Cityscapes和ADE20K数据集上取得了state-of-the-art的结果。 paper: https://arxiv.org/abs/1809.00916code: https://github.com/PkuRainBow/OCNet 主要贡献： 提出了逐像素的object context，包含了与像素P相同类别的objects的信息。 OCNet在Cityscapes和ADE20K数据集上取得了state-of-the-art的结果 OverViewOCNet以ResNet-101为backbone(后两个block为dilated convolution，dilation rate分别为2和4)，其后是这篇文章提出的object context module去计算逐像素的object context，最后是1×1的卷积输出预测图。object context module是基于Self Attention Mechanism，作者一共提出了3种不同的object context module: base-OC, Pyramid-OC, ASP-OC。 Object Context这里大致介绍了Object Context module中的变量和基本原理。X为输入的特征图，P为位置特征图，W为由self-attention得到的逐像素的相似度图，第i行表示所有像素与第i个像素的相关性，C为Object Context module最后得到的逐像素的object context表达。用公式表示为： c_{i}=\sum_{j=1}^{N}{w_{i,j}\phi\left ( x_{j} \right )}其中W可由以下公式得到： w_{i,j}=\Phi \left ( x_{i},x_{j},p_{i},p_{j} \right )进一步又可以写成： \bar{x}_{i} = x_{i}+p_{i} \quad w_{i,j}=\Phi \left ( \bar{x}_{i},\bar{x}_{j} \right )=\frac{exp\left ( f_{query}\left ( \bar{x}_{i} \right )^{T} f_{key}\left ( \bar{x}_{j} \right )\right )}{\mathbb{C}\left ( \bar{x}_{i} \right )}其中$\mathbb{C}\left ( \bar{x}_{i} \right )=\sum_{j=1}^{N}exp\left ( f_{query}\left ( \bar{x}_{i} \right )^{T} f_{key}\left ( \bar{x}_{j} \right )\right )$ Object Context Module这里介绍了3种不同的object context module: base-OC, Pyramid-OC, ASP-OC。每个object context module都含有3个阶段：(1)计算X；(2)计算C；(3)融合。 Base-OC(1)计算X: 3×3的卷积减少channel从2048到512(2)计算C: 用self-attention模块计算逐像素的attention map和object context(3)融合: concatenate C 和 X进一步用1×1的卷积减少channel维度为512 Pyramid-OC(受PSPNet的启发)(1)计算X: 3×3的卷积减少channel从2048到512(2)计算C: 在四个分支中分别用self-attention模块计算逐像素的attention map和object context。第一个分支把全部特征图作为输入，第二个分支把特征图分为2×2的子区域，每个子区域应用共享的transform，第三个和第四个分支把输入分为3×3和6×6的子区域，每个子区域的transform不共享。最后concatenate每个分支的结果并用1×1的卷积增加X的维度与object context的维度相等再进行concatenate(3)融合: 进一步用1×1的卷积减少channel维度为512 ASP-OC(受DeepLAbV3的ASPP启发)(1)计算X: 共有5个分支。分别为1×1的卷积，3×3的卷积，dilation rate为12的3×3卷积，dilation rate为24的3×3卷积，dilation rate为36的3×3卷积(2)计算C: 对每个分支的结果分别用self-attention模块计算逐像素的attention map和object context。实验发现再2，3，4，5分支中使用self-attention模块会有害performance(3)融合: concatenate C 和 X，进一步用1×1的卷积减少channel维度为512 ExperimentsInfluence of the Positional Feature Influence of the Key/Query transform Influence of the Self-attention in ASP-OC Influence of Auxiliary loss Influence of the OHEM, Ms, Flip, Validation set Comparison with Baseline Comparison with State-of-the-art Visualization of the object context]]></content>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Self Attention Mechanism</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time(BMVC 2018)]]></title>
    <url>%2F2018%2F08%2F31%2FContextNet-Exploring-Context-and-Detail-for-Semantic-Segmentation-in-Real-time%2F</url>
    <content type="text"><![CDATA[作者基于factorized convolution, network compression和pyramid representations提出了ContextNet来实时语义分割。ContextNet有两条分支，deep branch在较低的分辨率获取全局的context信息，而shallow branch聚焦于高分辨率的细节。在Cityscapes数据集实现了66.1%的精度，在1024×2048分辨率达到了183fps。paper: https://arxiv.org/abs/1805.04554 Network DesignCapturing context在deep branch采用了类似于MobileNet V2结构，输入为原始图像的1/4，由2个卷积层和12个bottleneck residual blocks组成，一共38层。 Spatial detailshallow branch由4层通道数为32，64，128，128卷积层组成，主要是为了改善局部context的结果 fusion unit最后在fusion unit将shallow branch和上采样4倍的deep branch的特征融合 辅助loss可以确保从global context提取到有意义的特征，所以在低分辨率的分支加入了辅助loss Experiments]]></content>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Real-Time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - ExFuse: Enhancing Feature Fusion for Semantic Segmentation(ECCV 2018 Fudan University, Megvii Inc.)]]></title>
    <url>%2F2018%2F08%2F31%2FExFuse-Enhancing-Feature-Fusion-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[这篇文章研究语义分割网络中高低层特征融合的效率问题。在很多工作中，融合不同尺度的特征是提高分割性能的一个重要手段。低层特征分辨率更高，包含更多位置、细节信息，但是由于经过的卷积更少，其语义性更低，噪声更多。高层特征具有更强的语义信息，但是分辨率很低，对细节的感知能力较差。如何将两者高效融合是改善分割模型的关键。 这篇文章从两个大的方向上提出了改善思路，即1.增加低层特征的语义和2.在高层特征中增加更多空间信息。paper: https://arxiv.org/abs/1805.04554 主要贡献： 从一个新的角度促进语义分割的性能，即通过更加有效地特征融合连接低级特征的分辨率和高级特征的语义差距 提出了新的网络ExFuse，在低级特征中引入了更多的语义信息，在高级特征中引入了更多的高空间分辨率信息 在 PASCAL VOC 2012 segmentation benchmark 达到了 state-of-the-art 语义分割网络中经常存在高低层特征的融合，但是这篇文章认为直接融合高底层特征会损害网络的性能。低级特征含有更多的空间信息，但是缺少语义信息；而高级特征由于降采样太多次含有较少的空间信息，但是富含语义信息。因此作者提出ExFuse网络，在低级特征中引入语义信息，在高级特征中引入空间信息。 Approach提出的ExFuse网络以GCN为基础，以ResNeXt101为backbone Introducing More Semantic Information into Low-level Features这篇文章中，作者为了增加低层特征的语义信息做了三点改进： 网络结构重排(layer rearrengement)，构建更适合于分割的预训练模型； 深度语义监督(semantic supervision)； 语义嵌入支路(semantic embedding branch)，将高层特征融入低层特征。 Layer rearrangementResNeXt网络结构中，各级的网络包含的残差单元个数为{3,4,23,3}。为了提高底层特征的语义性，一个想法便是让低层的两级网络拥有的层数更多。因此作者将残差单元个数重排为{8,8,9,8}，并重新在ImageNet上预训练模型。重排后网络的分类性能没有明显变化，但是分割模型可以提高约0.8个点(mean intersection over union)的性能。 Semantic Supervision (SS)深度语义监督其实在其他的一些工作里(如GoogLeNet，边缘检测的HED等等)已经使用到了，能够带来大约1个点的提升 Semantic Embedding Branch (SEB)其做法是将高层特征上采样后，与低层特征逐像素相乘，用在GCN之前。该部分能带来大约0.7个点的提升。 Embedding More Spatial Resolution into High-level Features这篇文章尝试将更多的空间特征融入到通道(channel)中去，包括: 通道分辨率嵌入(explicit channel resolution embedding)； 稠密邻域预测(densely adjacent prediction)。 Explicit Channel Resolution Embedding (ECRE)其思路是在上采样支路中使用子像素上采样模块(sub-pixel upsample)。这个结构能够让和空间信息相关的监督信息回传到各个通道中，从而让不同通道包含不同空间信息。该模块和原有的反卷积一起使用才能显示出更好的性能。同单独使用反卷积相比，性能可以提高约0.6个点。 Densely Adjacent Prediction (DAP)DAP模块只使用在输出预测结果的时候。其想法也是通过扩展通道数来增加空间信息。举一个例子来描述其功能，假设DAP的作用区域为3x3，输出结果的通道数为21，则扩展后的输出通道数为21x3x3。每3x3个通道融合成一个通道。如在最终结果中，第5通道(共21通道)的(12,13)坐标上的像素，是通过DAP之前的第5+0通道(11,12)、5+1通道的(11,13)、5+2通道的(11,14)、5+3通道的(12,12)、5+4通道的(12,13)、5+5通道的(12,14)…平均得到的。DAP能带来约0.6个点的提升。 最终各个模块累计评价 Experiment]]></content>
      <tags>
        <tag>Semantic Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation(ECCV 2018 Megvii Inc.)]]></title>
    <url>%2F2018%2F08%2F03%2FBiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[这篇论文针对语义分割任务中常常为了达到实时推理的速度而不得不舍弃一定的性能导致的矛盾提出了一个新的Bilateral Segmentation Network (BiSeNet)，其中首先设计了一条spatial path来保留空间信息生成高分辨率的特征，同时又有一条context path快速的降采样来获得充分的感受野，最后提出了特征融合模块有效地结合这两种特征。所提出的网络在Cityscapes，Cityscapes，COCO-Stuﬀ数据集实现了速度和精度的平衡。对于Cityscapes数据集输入2048×1024的图像实现了68.4%的mean IoU, 105FPS，比已经存在的方法实时性提高很多。 paper: https://arxiv.org/abs/1808.00897http://openaccess.thecvf.com/content_ECCV_2018/html/Changqian_Yu_BiSeNet_Bilateral_Segmentation_ECCV_2018_paper.htmlcode: 暂时未开源解读: https://zhuanlan.zhihu.com/p/41475332 主要贡献： 提出了Bilateral Segmentation Network (BiSeNet)，两条path分别生成高分辨率的特征和获得充分的感受野 提出了两个特殊的模块Feature Fusion Module (FFM)和 Attention Refinement Module (ARM) 在Cityscapes，Cityscapes，COCO-Stuﬀ数据集实现了速度和精度的平衡。 目前实时语义分割的三种方式 通过cropping或者resizing限制输入图像大小来降低计算复杂度如图(a)左侧 减少channel的数量如图(a)右侧 移除网络的后面的stage如图(a)右侧 为了弥补空间信息的损失，许多研究者利用了U形的架构如图(b)，但是作者认为这样不能从根本解决问题，U形的架构有两个缺点：1) 这种架构会减慢推理速度；2) 由于channle数减少损失的空间信息并不能很好地恢复 Bilateral Segmentation Network基于以上分析，作者提出了Bilateral Segmentation Network (BiSeNet)。BiSeNet含有两部分Spatial Path (SP) 和 Context Path (CP)。设计了一条spatial path来保留空间信息生成高分辨率的特征，同时又有一条context path快速的降采样来获得充分的感受野，最后提出了Feature FusionModule (FFM)和Attention Refinement Module (ARM)有效地结合这两种特征及进一步refine feature map。训练时采用了辅助的损失函数 Spatial path如图所示，Spatial path由3个stride为2的卷积层组成，因此输出的特征图为输入图像的1/8。由于特征图的大小较大因此保留了较多的空间信息 Context path由于pyramid pooling module, atrous spatial pyramid pooling 和 “large kernel”计算量和内存消耗较大,作者在Context path中利用了快速降采样的轻量级的pretrained模型Xception39和global average pooling来提供足够大的感受野。最后作者借鉴了UNet的想法，采取了不完全U形的decoder结构，上采样global average pooling和Xception的后两个阶段的特征图得到最后的特征图。 Attention refinement module在结合Context path中的特征图时，作者采用了SENet中的结构，对channel求不同的权重，如图(b) Feature fusion module在融合两个path的特征时使用了Feature fusion module。首先将两个path的特征concatenate，再做一次卷积，最后用SE block对channel上做attention，如图(c) Experimental ResultsAblation studyBaseline Ablation for U-shape Ablation for spatial pathAblation for attention refinement moduleAblation for feature fusion moduleAblation for global average pooling Speed and Accuracy AnalysisSpeed analysis Accuracy analysis]]></content>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Real-Time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - Pyramid Attention Network for Semantic Segmentation(BMVC 2018)]]></title>
    <url>%2F2018%2F06%2F20%2FPyramid-Attention-Network-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[论文作者将注意力机制和空间金字塔（spatial pyramid）相结合，来提取准确而密集的特征并获取像素标签。他们引入了一个特征金字塔注意力模块 (Feature Pyramid Attention module)，在高层的输出上施加空间金字塔注意力结构，并结合全局池化策略来学习更好的特征表征。此外，利用每个解码器层中的全局注意力上采样模块 (Global Attention Upsample module) 得到的全局上下文特征信息，作为低级别特征的指导，以此来筛选不同类别的定位细节。提出的方法在 PASCAL VOC 2012 数据集上实现了当前最佳的性能。而且无需经过 COCO 数据集的预训练过程，他们的模型在 PASCAL VOC 2012 和 Cityscapes 基准测试中能够实现了 84.0% mIoU。paper: https://arxiv.org/abs/1805.10180 1. Introduction&emsp;&emsp;作者在语义分割时主要考虑两个挑战。首先是多尺度目标的存在会加大语义分割任务中类别分类的困难。为了解决这个问题，PSPNet 或 DeepLab提出空间金字塔结构，旨在不同的网格尺度或扩张率下(称之为空间金字塔池化，ASPP)，融合多尺度的特征信息。在 ASPP 模块中，扩张卷积是一种稀疏计算，这可能会导致产生grid artifacts。而 PSPNet 中提出的金字塔池化模块则可能会丢失像素级别的定位信息。受SENet和Parsenet的启发，作者尝试从CNN的高层次特征中提取出准确的像素级注意力特征。&emsp;&emsp;另一个问题是，高层次的特征在对类别进行准确分类时非常有效，但在重组原始分辨率的二类预测问题方面比较薄弱。一些U型网络，如SegNet，Refinenet以及Large Kernel Matters能够在复杂的解码器模块中使用低层次信息来帮助高层次特征恢复图像细节。但是，这些方法都很耗时，运行效率不高。解决这个问题，作者提出了一种称为Global Attention Upsample (GAU)方法，这是一个有效的解码器模块，在不需要耗费过多计算资源的情况下，它可以提取高层次特征的全局上下文信息，作为低层次特征的加权计算的指导。作者的工作主要有以下三个贡献： 提出一个Feature Pyramid Attention(FPA)模块，可以在基于FCN的像素预测框架中嵌入不同尺度的上下文特征信息 开发了一个高效的解码器模块Global Attention Upsample，用于处理图像的语义分割问题 结合Feature Pyramid Attention模块和Global Attention Upsample模块，Pyramid Attention Network 在VOC2012和cityscapes的测试基准中取得了当前最佳的性能 2. Method2.1 Feature Pyramid Attention&emsp;&emsp;最近，PSPNet或DeepLab分别用空间金字塔池化和ASPP模块，但是dilated convolution可能会导致局部信息丢失或网格效应，这对于特征图的局部一致性是有害的。受到注意力机制的启发，作者考虑如何由高级特征提供精确的像素级的注意力。在当前的语义分割结构中，金字塔结构可以提取不同尺度的特征同时可以增加感受野，但是缺乏全局上下文的注意力取选择channle-wise的特征如SENet和EncNet一样。另一方面，channel-wise的attention并不足够取有效地提取多尺度特征且缺乏pixel-wise的信息。&emsp;&emsp;因此，作者提出了Feature Pyramid Attention(FPA)模块。模块能够融合来自 U 型网络 (如特征金字塔网络 FPN) 所提取的三种不同尺度的金字塔特征。为了更好地提取不同尺度下金字塔特征的上下文信息，作者在金字塔结构中分别使用3×3, 5×5, 7×7的卷积核。由于高层次特征图的分辨率较小，因此我们使用较大的内核并不会带来太多的计算负担。随后，金字塔结构逐步集成不同尺度下的特征信息，这样可以更准确地结合相邻尺度的上下文特征。然后，经过 1×1 卷积处理后，由 CNN 所提取的原始特征通过金字塔注意力特征进行逐像素相乘。此外，还引入了全局池化分支来联结输出的特征，这将进一步提高FPA模块的性能。得益于空间金字塔结构，FPA 模块可以融合不同尺度的上下文信息，同时还能为高层次的特征图提供更好的像素级注意力。不像PSPNet和ASPPconcatenate不同金字塔尺度的特征图，上下文信息和原始特征相乘是逐像素的，不会引入太多的计算量。 2.2 Global Attention Upsample&emsp;&emsp;由于PSPNet和Deeplab中的直接上采样和DUC中的one-step decoder缺乏不同尺度的低级特征图信息，可能对恢复空间定位精度有害。一般的encoder-decoder网络在decoder中逐渐地恢复尖锐地边界，但是大多数方法使用了复杂地decoder block，这消耗了大量地计算资源。最近研究表明将CNN和精心设计地金字塔模块可以获得很好的性能。作者认为decoder主要地特性为恢复类别像素地定位精度。而且含有大量的类别信息地高级特征可以用来给低级信息加入不同地权重去选择精确的位置细节信息。&emsp;&emsp;因此，作者提出的全局注意力上采样模块(Global Attention Upsample，GAU)，通过全局池化过程将全局上下文信息作为低层特征的指导，来选择类别的定位细节。具体地说，对低层次特征执行3×3 的卷积操作，以减少CNN特征图的通道数。从高层次特征生成的全局上下文信息依次经过1×1卷积、batch normalization和nonlinearity，然后再与低层次特征相乘。最后，高层次特征与加权后的低层次特征相加并进行逐步的上采样过程。GAU模块不仅能够更有效地适应不同尺度下的特征映射，还能以简单的方式为低层次的特征映射提供指导信息。 2.3 Network Architecture&emsp;&emsp;结合特征金字塔注意力模块 (FPA) 和全局注意力上采样模块 (GAU)，提出金字塔注意力网络 (PAN)。使用在 ImageNet 数据集上预训练好的 ResNet-101 模型，辅以扩张卷积策略来提取特征图。具体地说，在res5b模块上应用扩张率2的扩张卷积，以便ResNet输出的特征图大小为原输入图像的1/16，这与DeepLabv3+模型中的设置是一致的。正如PSPNet和DUC模型那样，用三个 3×3 卷积层来取代原ResNet-101模型中的7×7卷积。此外，使用FPA模块来收集ResNet的输出中密集的像素级注意力信息。结合全局的上下文信息，经GAU模块后，生成最终的预测图。 3. Experimental Results poly learning rate policy initial learning rate 4e-3 batch size 16 weight decay 0.0001 randomly left-right flipping and scaling between 0.5 and 2 3.1 Ablation Experiments3.1.1 Feature Pyramid Attention分别对池化类型、金字塔结构、卷积核大小、全局池化四种设置进行了 Ablation Experiments 分析，结果如下：其中 AVE 表示平均池化策略，MAX 表示最大池化，C333 代表全部使用 3×3 的卷积核，C357 表示所使用的卷积核分别为 3×3、5×5 和 7×7，GP 代表全局池化分支，SE 表示使用 SENet 注意力模块。 池化类型：发现 AVE 的性能要优于 MAX：对于 3×3 的卷积核设置，AVE 的性能能达到 77.54%，优于 MAX 所取得的77.13%。 金字塔结构：在验证集上能取得 72.60％ 的 mIoU。此外，使用 C333 和 AVE 时，模型的性能能够从 72.6％ 提升至 77.54％。还使用 SENet 注意力模块来取代金字塔结构，进一步对比评估二者的性能。实验结果表明，与 SENet 注意力模块相比，C333 和 AVE 设置能将性能提高了近1.8％。 卷积核大小：对于使用平均池化的金字塔结构，使用 C357 取代 C333 卷积核设置，金字塔结构中特征映射的分辨率为 16×16，8×8，4×4。实验结果表明，模型性能能够从 77.54％ 提高至 78.19％。 全局池化：进一步在金字塔结构中添加全局池化分支以提高模型性能。实验结果表明，在最佳设置下模型能够取得 78.37 的 mIoU 和 95.03% 的 Pixel Acc。 3.1.2 Global Attention Upsample首先，评估 ResNet101+GAU 模型，然后将 FPA 和 GAU 模块结合并在 VOC 2012 验证集中评估我们的模型。分别在三种不同的解码器设置下评估模型：(1) 仅使用跳跃连接的低级特征而没有全局上下文注意力分支。(2) 使用 1×1 卷积来减少 GAU 模块中的低层次特征的通道数。(3) 用 3×3 卷积代替 1×1 卷积减少通道数。此外，还比较了ResNet101+GAU 模型、Global Convolution Network 和 Discriminate Feature Network。 3.2 PASCAL VOC 2012结合 FPA 模块和 GAU 模块的最佳设置，在 PASVAL VOC 2012 数据集上评估了金字塔注意力网络 (PAN)。实验结果如表4、表5所示。可以看到，PAN 取得了84.0% mIoU，超过现有的所有方法。 3.3 Cityscapes]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Channel Attention mechanism</tag>
        <tag>Feature Pyramid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - Learning a Discriminative Feature Network for Semantic Segmentation (CVPR2018)]]></title>
    <url>%2F2018%2F06%2F19%2FLearning-a-Discriminative-Feature-Network-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[为了处理语义分割中仍然存在的两个问题：类内不一致和类间无差别，提出了Discriminative Feature Network (DFN)，DFN包含2个子网络：Smooth Network和Border Network。其中为了处理类内不连续，在Smooth Network加入了Channel Attention Block和global average pooling来选择更加有判别力的特征；用Border Network使得边界的特征更加有判别力。在PASCAL VOC 2012上实现了86.2%的 mean IOU，在Cityscapes上实现了80.3%的mean IOU。Uses deep supervision and attempts to process the smooth and edge portions of the segments separately paper: https://arxiv.org/abs/1804.09337http://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Learning_a_Discriminative_CVPR_2018_paper.html 1. Introduction大多数现有的方法所到的特征不具有判别力来区分以下两种情况： 同样的label但是看起来不同，即类内不一致，如图1第一行 相邻的patch看起来一样但是label不同，即类间无差别，如图1第二行 从宏观角度考虑，并没有将语义分割看作逐像素的标注问题，而是将每一类的像素作为一个整体。为了达到这个目的，提出了Discriminative Feature Network (DFN)。DFN包括两个子网络：Smooth Network和Border Network。Smooth Network用来解决类内不一致问题。为了学到更加有判别力的特征，Smooth Network考虑了两个关键因素： 一方面，需要多尺度和全局上下文特征来编码局部和全局的信息。如图1(a) 另一方面，随着多尺度信息的引入，对于某个尺度的物体，特征有不同程度的判别力，有些特征可能会预测错误。因此需要选择有判别力和有效的特征。 Smooth Network采用U形的结构来捕获多尺度上下文信息和，采用global average pooling来捕获全局上下文；采用Channel Attention Block(CBA)逐阶段的利用高级特征去指导低级特征的选择。 Border Network用来区分看起来很相似但是label不同的问题。大多数已经存在的方法将语义分割任务作为密集标注问题，这就忽略了明显地建模类间地关系。在在训练Border Network中整合了semantic boundary loss去学习有判别力地特征去扩大类间地区别。 2. Method 2.1 Smooth network&emsp;&emsp;在语义分割中有时会有不正确地预测结果，尤其是大区域部分或者含有复杂地场景，这就是类内不一致问题。导致类内不一致问题主要是由于缺乏上下文。因此在网络最后加入了global average pooling来获得全局上下文。然而全局上下文信息仅仅具有高度地语义信息，对于恢复空间信息没有帮助。因此，正如大多数已经存在地方法，需要用多尺度信息逐渐恢复空间信息。然而不同尺度地特征有不同程度地判别力，有可能会导致错误分类，所以需要选择更加有判别力地特征来预测label。&emsp;&emsp;DFN采用ResNet作为基础网络。模型根据特征图大小可以分为5个阶段，每个阶段有不同地识别能力。在lower stage，网络可以很好地编码空间信息，但是由于感受野较小且没有空间上下文地引导，具有较差的语义一致性。在high stage，由于有较大的感受野具有较强的语义一致性，但是预测结果较为粗糙。整体来说，lower stage有精确的空间信息，higher stage有精确的语义信息。所以作者提出Smooth network，Smooth network利用了high stage 的一致性去保证low stage来得到最佳的预测。&emsp;&emsp;作者提到目前有两种语义分割结构： Backbone-Style，如PSPNet和DeepLab v3 Encoder-Decoder-Style，如RefineNet，Global Convolution Network &emsp;&emsp;作者指出Encoder-Decoder-Style利用了内在的不同阶段的多尺度信息，但是缺乏含有强烈一致性的全局上下文信息；而且网络通过求和结合不同阶段的特征，这忽视了不同阶段含有不同的一致性。为了补救这种影响，首先加入global average pooling将U形网络转为V形网络，且global average pooling引入了强烈的一致性约束。为了强化这种一致性设计了Channel Attention Block(CAB)，用相邻阶段的特征去计算channel的权重，如图2(c)和3。high stage的特征提供了强烈的一致性保证，low stage加入了不同判别力的信息。在decoder阶段除了使用Channel Attention Block(CAB)外，还使用了Reﬁnement residual block用来优化特征，如图2(b)。首先是一个1×1的卷积统一通道数为512，然后接着是一个基本的残差block。 2.2 Border network&emsp;&emsp;在语义分割任务中，有时两个区域由于看起来相似而预测错误，尤其是这两个区域相邻的时候。因此为了放大在特征上的区别，采用了语义边界来指导特征学习。为了提取语义边界，作者提出了Border network，Border network采用了显式地监督方式，直接学习语义边界，扩大了类间特征地区分性。Border network为bottom-up结构，在low stage可以获得精确地边缘信息，在high stage可以获得语义信息，它消除了有语义信息的边缘。Border network的ground truth可以在分割label ground truth用传统的边缘检测方法得到，如Canny。为了弥补正负样本不平衡的问题，采用了目标检测中的focal loss。 2.3 Network Architecture&emsp;&emsp;使用预训练的ResNet作为基础网络。在Smooth network中，在ResNet网络最后加入global average pooling层得到最强的一致性，利用Channel Attention Block去改变channel的权重进一步增加一致性。在Border network以显式地语义边界监督地方式，网络获得精确地语义边界且使得两边地特征更加有区分性。在Smooth network中使用softmax loss，在Border network中使用focal loss，为了平衡loss，在语义边界损中乘以$\lambda$。 3. Experimental Results3.1 Implementation details mini-batch stochastic gradient descent (SGD) batch size 32 momentum 0.9 weight decay 0.001 learning rate policy: poly, power 0.9, initial learning rate 0.004 $\lambda$为0.1 randomly horizontal flip and randomly scale {0.5，0.75，1，1.5，1.75} 3.2 Ablation study3.2.1 Smooth network以ResNet-101为baseline，在resNet-101中加入随机尺度缩放可以从69.26%提高到72.86%然后再ResNet中逐渐加入RRB: refinement residual block，GP: global pooling branch，CAB: channel attention block，DS: deep supervision的消融实验，最高可达到7954%Smooth Network在PASCAL VOC 2012数据集上的视觉效果 3.2.2 Border network在Smooth Network网络在加入BN: Border Network和MS Flip: Adding multi-scale inputs and left-right flipped inputs可达到80.01%在Smooth Network中加入Border Network的视觉效果Border Network所预测的边界图 3.2.3 Discriminative Feature network不同的$\lambda$的值对结果的影响，最后取$\lambda$为0.1DFN每个stage的语义分割结果和语义边界结果在PASCAL VOC 2012上的结果在Cityscapes上的结果]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Channel Attention mechanism</tag>
        <tag>Boundary Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows通过ssh连接内网Ubuntu]]></title>
    <url>%2F2018%2F06%2F19%2FWindows%E9%80%9A%E8%BF%87ssh%E8%BF%9E%E6%8E%A5%E5%86%85%E7%BD%91Ubuntu%2F</url>
    <content type="text"><![CDATA[Windows通过ssh反向连接内网Ubuntu 机器号 IP 用户名 备注 A A.A.A.A userA 目标服务器，在局域网中 B B.B.B.B userB 代理服务器，在外网中，无法访问 A C - - 可以直接访问 B，无法直接访问 A 目标从 C 机器使用 SSH 访问 A 解决方案在 A 机器上做到 B 机器的反向代理；在 B 机器上做正向代理本地端口转发 环境需求每台机器上都需要 SSH 客户端A、B 两台机器上需要 SSH 服务器端。通常是 openssh-server。 在 Ubuntu 上安装过程为1sudo apt-get install openssl-server 实施步骤1. 建立 A 机器到 B 机器的反向代理【A 机器上操作】命令：1ssh -fCNR &lt;port_b1&gt;:localhost:22 userB@B.B.B.B -f 后台执行ssh指令 -C 允许压缩数据 -N 不执行远程指令 -R 将远程主机(服务器)的某个端口转发到本地端指定机器的指定端口 ：建立在B机器上，用来代理设备A机器22端口的端口 userB@B.B.B.B ：B机器的用户名和IP地址 例如：ssh -fCNR 1314:localhost:22 userB@B.B.B.B 在设备A上可以使用ps aux | grep ssh查看在B设备上可以使用netstat -lntp 1314指令来查看此时B设备上已经可以通过ssh -p 1314 userA@localhost连接到设备A 2. 建立 B 机器上的正向代理，用作本地转发。做这一步是因为绑定后的端口只支持本地访问【B 机器上操作】需要在服务器上配置 GatewayPorts 使其可以正常工作（如果不配置这一项会导致你在之后运行的命令后会发现只有在服务器上才可以访问） 编辑sudo vi /etc/ssh/sshd_config在最后一行添加GatewayPorts yes 重新启动服务sudo service sshd restart1ssh -fCNL *:&lt;port_b2&gt;:localhost:&lt;port_b1&gt; localhost 例如：ssh -fCNL *:1994:localhost:1021 userB@localhost 为本地转发端口，用以和外网通信，并将数据转发到 ，实现可以从其他机器访问。其中的*表示接受来自任意机器的访问。 3. 现在在 C 机器上可以通过 B 机器 ssh 到 A 机器1ssh -p &lt;portb2&gt; userA@B.B.B.B 在xshell中不能执行以上命令，而应该配置session，主机为B.B.B.B，端口为&lt;portb2&gt;，用户名密码为A机器 用autossh建立稳定的反向代理不幸的是这种ssh反向连接会因为超时或NAT地址映射的转变而关闭，如果关闭了那从外网连通内网的通道就无法维持了，为此我们需要另外的方法来提供稳定的ssh反向代理隧道。 ssh每次重连都需要键入密码，故在此首先设置从机器A免密码登录到机器B 首先在内网机器A上配置ssh秘钥：1ssh-keygen -t rsa -C "your@email.com" 邮箱自行更换，然后一路回车下去。 然后利用如下命令将A的SSH秘钥添加到B的authorized_keys里面：1ssh-copy-id userB@B.B.B.B 执行后会提示输入主机 B 的密码，执行完毕之后，我们登录到 B，就发现 authorized_keys 里面就多了 A 的 SSH 公钥了，成功建立 SSH 认证。那以后这台内网的A机器ssh登陆我外网的B机器就可以免密码登陆啦~检验是否已经可以使用免密码登陆可以使用如下指令来检验：1ssh userB@B.B.B.B 然后在内网A机器的配置：1sudo apt-get install autossh 然后执行如下命令即可完成反向 SSH 配置：1autossh -M 7043 -fCNR &lt;port_b1&gt;:localhost:22 userB@B.B.B.B 这里 -M 后面任意填写一个可用端口即可，-N 代表只建立连接，不打开shell ，-f 代表建立成功后在后台运行，-R 代表指定端口映射。 这里是将 A 主机的 22 端口映射到 B 主机的 端口，这样就完成了配置。我们可以在机器A或机器B上面用如下命令来检查autossh建立的反向代理是否成功：1netstat -antpul | grep ':&lt;port_b1&gt;' 若建立成功则有相应的输出，否则无输出. 只要我们再访问 B 主机的 端口，就会自动转发到 A 主机的 22 端口了，即可以公网访问了。 然后连接我们的公网ip： ssh userB@B.B.B.B -p &lt;port_b2&gt;然后输入密码就可以通过公网连接我们的内网了。最后配置在Linux上配置开机自动启动autossh，免去了重启Linux后要自己启动的autossh的麻烦输入:12sudo touch /etc/systemd/system/remote-autossh.servicesudo gedit /etc/systemd/system/remote-autossh.service 添加内容:123456789101112[Unit]Description=Robust SSH Reverse Tunnel and Port ForwardAfter=network.target[Service]User=zhangbinExecStart=/usr/bin/autossh -M 7043 -q -o "ServerAliveInterval 60" -o "ServerAliveCountMax 6" -o "StrictHostKeyChecking no" -fCNR 1314:localhost:22 userB@B.B.B.B KillMode=process[Install]WantedBy=multi-user.targetAlias=autossh.service To make it executable:1234sudo chmod +x /etc/systemd/system/remote-autossh.servicesudo systemctl enable remote-autossh.servicesudo systemctl start remote-autossh.servicesudo systemctl status remote-autossh.service]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ssh</tag>
        <tag>反向代理</tag>
        <tag>linux</tag>
        <tag>远程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - Vortex Pooling: Improving Context Representation in Semantic Segmentation]]></title>
    <url>%2F2018%2F06%2F14%2FVortex-Pooling-Improving-Context-Representation-in-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[作者认为在预测一个像素的类别时，靠近靠近这个像素的区域比相对较远的区域更加重要。因此提出了Vortex Pooling来有效的上下文信息。用Vortex Pooling取代DeepLab v3中的ASPP muodule后在PASCAl VOC 2012验证集上超过DeepLab v3 1.5%，在测试集上超过0.6%。 1. DeepLab v3 recap&emsp;&emsp;在DeepLab v3中使用了ResNet-101和ASPP module分别进行feature extraction和feature aggregation。&emsp;&emsp;ASPP mudule中是由一个1×1的卷积，3个3×3的卷积和平均值池化组成的。&emsp;&emsp;ASPP mudule的输入为h×w×c的特征图，作者这里将特征图看作h×w个c维的descriptor，在分类一个像素是使用了25个descriptor，计算方式如下：&emsp;&emsp;1×1的卷积使用了1个descriptor，3个3×3的卷积使用了(3×3)×3=27个descriptor，但是中心像素在1×1的卷积时已经算过了，因此需要减去3，所以在ASPP module一共使用了(1+27)-3=25个descriptor&emsp;&emsp;随后作者定义了descriptor的利用率：$r=\frac{u}{hw}$&emsp;&emsp;在输入特征图为513×513的情况下，ASPP module的输入特征图h和w都为65，所以ASPP的利用率为$r=\frac{25}{65×65}\approx0.0059$，这意味着ASPP module只使用了所有descriptor的0.59%，所以ASPP module只使用了所有descriptor的一小部分，这会丢失一些重要的上下文信息。 &emsp;&emsp;因此，作者的这篇论文的目的是改进feature aggregation的方法。一方面，在聚合局部和上下文信息时，应该考虑尽可能多的descriptor，这可以使逐像素的分类得到全面的上下文特征；另一方面，虽然需要更多的descriptor，但是应该区别对待descriptor，因为靠近target pixel的区域一般包含更多的语义信息，可以得到一个好的特征表达，对于那些远离target pixel的区域，粗略的表达就足够上下文信息。因此作者提出了两个有更高的利用率的module。 2. Vortex Pooling 2.1 Module A&emsp;&emsp;在Module A中首先对特征图用k×k的平均值池化，然后使用四个不同dilation rate为1，12，24，36的卷积层，然后聚合得到descriptor。Module A的利用率的越是$k^2$。 2.2 Module B&emsp;&emsp;由于在Module A中同样对待所有的descriptor而不管是否靠近target pixel，作者又提出了Module B，也把它叫做Vortex Pooling。在Module B中首先分别对特征图采用3×3，9×9，27×27的平均值池化，然后使用四个不同dilation rate为1，3，9，27的卷积层，然后聚合得到descriptor。1，3，9，27这样的几何序列不仅对不同的子区域的关注不同，而且后续还可以更加高效的执行。当h和w小于81时，Module B的利用率为1，在实际中h和w小于81很容易达到。 2.3 Accelerate Vortex Pooling&emsp;&emsp;Vortex Pooling中包含了pooling操作和大的卷积核，效率不够高。因此作者提出了Module C来加速Vortex Pooling。具体做法为在对特征图X进行3×3的池化后得到Y1，计算Y2时不对X做9×9的池化，而对Y1做3×3的池化得到Y2，同理对对Y2做3×3的池化得到Y3。所以在Module B中使用了一个3×3的池化，一个9×9的池化，一个9×9的池化。而在Module C中使用了3个3×3的池化。实验表明使用Module C时测试速度和Deeplab v3相似但是有更高的精度。 3. Experimental Results3.1 Ablation Studies这部分使用ResNet50做实验 Learning rate policy：poly 初始学习率0.007，30000次迭代 Data Augmentation：随机缩放图像0.5倍到2倍，随机翻转 Crop Size：513×513 Multi-grid：在block4中dilation rate为(1,2,4)来减轻网格效应 Inference strategy on the val set：通过在ResNet的后两个block中使用dilated convolution使output stride为8 Batch Normalization：batchsize为16，同样使用了EncNet中的synchronized batch nomalization layer &emsp;&emsp;实验发现在Module A中使用5×5的池化比ASPP精度高，但是当进一步增加pooling size时，不能进一步增加精度。如虽然9×9的池化的利用率为0.479比5×5的池化利用率0.148高，但是9×9的池化更差，作者认为对于靠近target pixel的区域，9×9的池化太大而丢失了细节；对于远离target pixel的区域，9×9的池化太大小而不能全面的理解上下文。而在Module B中利用不同的池化大小可以避免这个问题，小的池化可以建立对于细节的特征表达，大的池化可以帮助得到粗略的上下文信息。 3.2 Compared with State-of-the-Art整体网络结构如下，以ResNet101为基础网络，取代了DeepLab v3中的ASPP为Vortex Pooling，同时也使用了一个全局平均值池化来整合全局特征，最后通过1×1的卷积得到最终的预测图。]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遥感数据集]]></title>
    <url>%2F2018%2F06%2F12%2F%E9%81%A5%E6%84%9F%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[整理了遥感中的数据集，长期更新！！ webhttp://www.jhuapl.edu/pubgeo/pubgeo.htmlhttp://www2.isprs.org/commissions/comm2/wg6/bench.htmlhttp://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#remote Image classification Datasets Images per class Scene class Total images Spatial resolution(m) Image sizes Year UC Merced Land Use 100 21 2100 0.3 256×256 2010 WHU-RS19 ~50 19 1005 up to 0.5 600×600 2010 RSSCN7 400 7 2800 - 400×400 2015 RSC11 ~100 11 1232 0.2 512×512 2016 SIRI-WHU 200 12 2400 2 200×200 2016 AID 200~400 30 10000 0.5~0.8 600×600 2017 EuroSAT 2000 2500 3000 10 27000 - 64×64 2017 NWPU-RESISC45 700 45 31500 ~30 to 0.2 256×256 2017 PatternNet 800 38 30400 0.062~4.693 256×256 2017 RSI-CB RSI-CB128(~800)RSI-CB256(~690) RSI-CB128(45)RSI-CB256(35) RSI-CB128(36707)RSI-CB256(24747) 0.3~3 128×128256×256 2017 1. UC Merced Land Use Datasethttp://vision.ucmerced.edu/datasets/landuse.htmlThis is a 21 class land use image dataset meant for research purposes.There are 100 images for each of the following classes:agricultural，airplane，baseballdiamond，beach，buildings，chaparral，denseresidential，forest，freeway，golfcourse，harbor，intersection，mediumresidential，mobilehomepark，overpass，parkinglot，river，runway，sparseresidential，storagetanks，tenniscourtEach image measures 256x256 pixels.The images were manually extracted from large images from the USGS National Map Urban Area Imagery collection for various urban areas around the country. The pixel resolution of this public domain imagery is 1 foot.Reference： Yi Yang and Shawn Newsam, “Bag-Of-Visual-Words and Spatial Extensions for Land-Use Classification,” ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM GIS), 2010. 2. WHU-RS19http://captain.whu.edu.cn/repository.htmlhttp://captain.whu.edu.cn/datasets/WHU-RS19.zipWHU-RS19是从谷歌卫星影像上获取19类遥感影像，可用于场景分类和检索。Reference： G.-S. Xia, W. Yang, J. Delon, Y. Gousseau. H. Maitre, H. Sun, “Structural high-resolution satellite image indexing”. Symposium: 100 Years ISPRS - Advancing Remote Sensing Science: Vienna, Austria, 2010 3. RSSCN7https://sites.google.com/site/qinzoucn/documentsThis dataset contains 2800 remote sensing images which are from 7 typical scene categories - the grass land, forest, farm land, parking lot, residential region, industrial region, and river&amp;lake. For each category, there are 400 images collected from the Google Earth which are sampled on 4 different scales with 100 images per scale. Each image has a size of 400*400 pixels. This dataset is rather challenging due to the wide diversity of the scene images which are captured under changing seasons and varying weathers, and sampled with different scales.Reference： Qin Zou, Lihao Ni, Tong Zhang and Qian Wang, Deep learning based feature selection for remote sensing scene classification, IEEE Geoscience and Remote Sensing Letters, vol. 12, no. 11, pp.2321-2325, 2015. 4. SAT-4 and SAT-6 airborne datasetshttp://csc.lsu.edu/~saikat/deepsat/https://arxiv.org/abs/1509.03602Images were extracted from the National Agriculture Imagery Program (NAIP) dataset. The NAIP dataset consists of a total of 330,000 scenes spanning the whole of the Continental United States (CONUS). We used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ~6000 pixels in width and ~7000 pixels in height, measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ~65 terabytes. The imagery is acquired at a 1-m ground sample distance (GSD) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points. The images consist of 4 bands - red, green, blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class. Once labeled, 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28x28 as the window size to maintain a significantly bigger context, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch.The datasets are encoded as MATLAB .mat files that can be read using the standard load command in MATLAB. Each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared. The training and test labels are 1x4 and 1x6 vectors for SAT-4 and SAT-6 respectively having a single 1 indexing a particular class from 0 through 4 or 6 and 0 values at all other indices.Reference: Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert Dibiano, Manohar Karki and Ramakrishna Nemani, DeepSat - A Learning framework for Satellite Imagery, ACM SIGSPATIAL 2015. 5. RSC11https://www.researchgate.net/publication/271647282_RS_C11_Database611 scenes, all using high resolution remote sensing images, downloaded from Google Earth 6. SIRI-WHUhttp://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.htmlThis is a 12-class Google image dataset of SIRI-WHU meant for research purposes.There are 200 images for each of the following classes:Agriculture, Commercial, Harbor, Idle land, Industrial, Meadow, Overpass, Park, Pond, Residential, River, WaterEach image measures 200*200 pixels, with a 2-m spatial resolution.This dataset was acquired from Google Earth (Google Inc.) and mainly covers urban areas in China, and the scene image dataset is designed by RS_IDEA Group in Wuhan University (SIRI-WHU).Reference: B. Zhao, Y. Zhong, G.-s. Xia, and L. Zhang, “Dirichlet-Derived Multiple Topic Scene Classification Model Fusing Heterogeneous Features for High Spatial Resolution Remote Sensing Imagery,” IEEE Transactions on Geoscience and Remote Sensing, vol. 54, no. 4, pp. 2108-2123, Apr. 2016. B. Zhao, Y. Zhong, L. Zhang, and B. Huang, “The Fisher Kernel Coding Framework for High Spatial Resolution Scene Classification,” Remote Sensing, vol. 8, no. 2, p. 157, doi:10.3390/rs8020157 2016. Q. Zhu, Y. Zhong, B. Zhao, G.-S. Xia, and L. Zhang, “Bag-of-Visual-Words Scene Classifier with Local and Global Features for High Spatial Resolution Remote Sensing Imagery,” IEEE Geoscience and Remote Sensing Letters, DOI:10.1109/LGRS.2015.2513443 2016. 7. AIDhttp://www.lmars.whu.edu.cn/xia/AID-project.htmlhttps://captain-whu.github.io/AIDAID is a new large-scale aerial image dataset, by collecting sample images from Google Earth imagery. Note that although the Google Earth images are post-processed using RGB renderings from the original optical aerial images, it has proven that there is no significant difference between the Google Earth images with the real optical aerial images even in the pixel-level land use/cover mapping. Thus, the Google Earth images can also be used as aerial images for evaluating scene classification algorithms.The new dataset is made up of the following 30 aerial scene types: Aairport, bare land, baseball field, beach, bridge, center, church, commercial, dense residential, desert, farmland, forest, industrial, meadow, medium residential, mountain, park, parking, playground, pond, port, railway station, resort, river, school, sparse residential, square, stadium, storage tanks and viaduct. All the images are labelled by the specialists in the field of remote sensing image interpretation, and some samples of each class are shown in Fig. In all, the AID dataset has a number of 10000 images within 30 classes.Reference: G.-S. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong, L. Zhang, X. Lu, “AID: A benchmark dataset for performance evaluation of aerial scene classification”, IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 7, pp. 3965-3981, 2017. 8. NWPU-RESISC45http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.htmlhttps://arxiv.org/abs/1703.00121NWPU-RESISC45 dataset is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class. These 45 scene classes include airplane, airport, baseball diamond, basketball court, beach, bridge, chaparral, church, circular farmland, cloud, commercial area, dense residential, desert, forest, freeway, golf course, ground track field, harbor, industrial area, intersection, island, lake, meadow, medium residential, mobile home park, mountain, overpass, palace, parking lot, railway, railway station, rectangular farmland, river, roundabout, runway, seaice, ship, snowberg, sparse residential, stadium, storage tank, tennis court, terrace, thermal power station, and wetland.Reference: G. Cheng, J. Han, X. Lu. Remote Sensing Image Scene Classification: Benchmark and State of the Art. Proceedings of the IEEE. 9. PatternNethttps://sites.google.com/view/zhouwx/datasetPatternNet is a large-scale high-resolution remote sensing dataset collected for remote sensing image retrieval. There are 38 classes and each class has 800 images of size 256×256 pixels. The images in PatternNet are collected from Google Earth imagery or via the Google Map API for some US cities. The figure shows some example images from each class.Reference: Zhou, W., Newsam, S., Li, C., &amp; Shao, Z. (2017). PatternNet: A Benchmark Dataset for Performance Evaluation of Remote Sensing Image Retrieval. arXiv preprint arXiv:1706.03424. Zhou, W., Newsam, S., Li, C., &amp; Shao, Z. (2017). Learning Low Dimensional Convolutional Neural Networks for High-Resolution Remote Sensing Image Retrieval. Remote Sensing, 9(5), 489. Zhou, W., Shao, Z., Diao, C., &amp; Cheng, Q. (2015). High-resolution remote-sensing imagery retrieval using sparse features by auto-encoder. Remote Sensing Letters, 6(10), 775-783. 10. RSI-CBhttps://arxiv.org/abs/1705.10450https://github.com/lehaifeng/RSI-CB Considering the different image size requirements of the DCNN, construct two datasets of 256 × 256 and 128 × 128 pixel sizes (RSI-CB256 and RSI-CB128, respectively) with 0.3–3-m spatial resolutions. The former contains 35 categories and more than 24,000 images. The latter contains 45 categories and more than 36,000 images. We establish a strict object category system according to the national standard of land-use classification in China and the hierarchical grading mechanism of ImageNet. The six categories are agricultural land, construction land and facilities, transportation and facilities, water and water conservancy facilities, woodland, and other land.Reference: Li H, Tao C, Wu Z, et al. RSI-CB: a large scale remote sensing image classification benchmark via crowdsource data[J]. arXiv preprint arXiv:1705.10450, 2017. 11. AID++https://arxiv.org/abs/1806.00801Reference: Jin P, Xia G S, Hu F, et al. AID++: An Updated Version of AID on Scene Classification[C]//IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2018: 4721-4724. 12. BigEarthNethttps://arxiv.org/abs/1902.06148http://bigearth.net/The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. To construct the BigEarthNet, 125 Sentinel-2 tiles acquired between June 2017 and May 2018 over the 10 countries (Austria, Belgium, Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, Switzerland) of Europe were initially selected. All the tiles were atmospherically corrected by the Sentinel-2 Level 2A product generation and formatting tool (sen2cor). Then, they were divided into 590,326 non-overlapping image patches. Each image patch was annotated by the multiple land-cover classes (i.e., multi-labels) that were provided from the CORINE Land Cover database of the year 2018 (CLC 2018).Reference: G. Sumbul, M. Charfuelan, B. Demir, V. Markl, BigEarthNet: A large-scale benchmark archive for remote sensing image understanding, arXiv preprint, 2019. Object detection Dataset # Class # Images # Image width # Instances Annotation way RSOD 4 976 6950 horizontal BB NWPU VHR-10 10 800 ~1000 3651 horizontal BB VEDAI 3 1268 512,1024 2950 oriented BB COWC 1 53 2000~19,000 32716 one dot HRSC2016 1 1061 ~1100 2976 oriented BB DOTA 15 2806 800~4000 118,282 oriented BB xView 60 1128 ~1,000,000 horizontal BB 1. RSOD-Datasethttps://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-It is an open dataset for object detection in remote sensing images. The dataset includes aircraft, oiltank, playground and overpass.The format of this dataset that for PASCAL VOC.The datase includes 4 files, and each file is for one kind of object. Please download the dataset files from BaiduYun. aircraft dataset, 4993 aircrafts in 446 images. playground, 191 playgrounds in 189 images. overpass, 180 overpass in 176 overpass. oiltank, 1586 oiltanks in 165 images. Reference: Y. Long, Y. Gong, Z. Xiao and Q. Liu, “Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks,” in IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 5, pp. 2486-2498, May 2017. doi: 10.1109/TGRS.2016.2645610, link Z Xiao, Q Liu, G Tang, X Zhai, “Elliptic Fourier transformation-based histograms of oriented gradients for rotationally invariant object detection in remote-sensing images”, International Journal of Remote Sensing, vol. 36, no. 2, 2015 2. NWPU VHR-10 datasetNWPU VHR-10 dataset is a publicly available 10-class geospatial object detection dataset used for research purposes only.These ten classes of objects are airplane, ship, storage tank, baseballdiamond, tennis court, basketball court, ground track field, harbor, bridge,and vehicle. This dataset contains totally 800 very-high-resolution (VHR)remote sensing images that were cropped from Google Earth and Vaihingen dataset and then manually annotated by experts.Thisdataset can be downloaded from OneDrive or BaiduWangpan.Reference: G. Cheng, J. Han, P. Zhou, L. Guo. Multi-class geospatial object detection and geographic imageclassification based on collection of part detectors. ISPRS Journal ofPhotogrammetry and Remote Sensing, 98: 119-132, 2014. G. Cheng, J. Han. A survey on objectdetection in optical remote sensing images. ISPRS Journal of Photogrammetry andRemote Sensing, 117: 11-28, 2016. G. Cheng, P. Zhou, J. Han. Learningrotation-invariant convolutional neural networks for object detection in VHRoptical remote sensing images. IEEE Transactions on Geoscience and RemoteSensing, 54(12): 7405-7415, 2016. 3. Vehicle Detection in Aerial Imagery (VEDAI)https://downloads.greyc.fr/vedai/VEDAI is a dataset for Vehicle Detection in Aerial Imagery, provided as a tool to benchmark automatic target recognition algorithms in unconstrained environments. The vehicles contained in the database, in addition of being small, exhibit different variabilities such as multiple orientations, lighting/shadowing changes, specularities or occlusions. Furthermore, each image is available in several spectral bands and resolutions. A precise experimental protocol is also given, ensuring that the experimental results obtained by different people can be properly reproduced and compared. We also give the performance of some baseline algorithms on this dataset, for different settings of these algorithms, to illustrate the difficulties of the task and provide baseline comparisons.Reference: Vehicle Detection in Aerial Imagery: A small target detection benchmark., Sébastien Razakarivony and Frédéric Jurie, Journal of Visual Communication and Image Representation, 2015 4. Cars Overhead With Context(COWC)https://gdo152.llnl.gov/cowc/Poster PaperGithub: https://github.com/LLNL/cowcFTP: ftp://gdo152.ucllnl.org/cowc/The Cars Overhead With Context (COWC) data set is a large set of annotated cars from overhead. It is useful for training a device such as a deep neural network to learn to detect and/or count cars. The dataset has the following attributes: (1) Data from overhead at 15 cm per pixel resolution at ground (all data is EO). (2) Data from six distinct locations: Toronto Canada, Selwyn New Zealand, Potsdam and Vaihingen Germany, Columbus and Utah United States. (3) 32,716 unique annotated cars. 58,247 unique negative examples. (4) Intentional selection of hard negative examples. (5) Established baseline for detection and counting tasks. (6) Extra testing scenes for use after validation. Reference: Mundhenk T N, Konjevod G, Sakla W A, et al. A large contextual dataset for classification, detection and counting of cars with deep learning[C]//European Conference on Computer Vision. Springer, Cham, 2016: 785-800. 5. High resolution ship collections 2016 (HRSC2016)https://sites.google.com/site/hrsc2016/http://www.escience.cn/people/liuzikun/DataSet.htmlAll the images are collected from six famous harbors. The image resolutions are between 2-m and 0.4-m. The image sizes range from 300 to 1500 and most of them arelarger than 1000 x 600.Training, validation and test set contains 436 images including 1207 samples, 181 images including 541 samples and 444 images including 1228 samples respectively.Reference: Liu Z, Yuan L, Weng L, et al. A High Resolution Optical Satellite Image Dataset for Ship Recognition and Some New Baselines[C]//ICPRAM. 2017: 324-331. 6. DOTA: A Large-scale Dataset for Object Detection in Aerial ImagesDOTA Dataset Page:https://captain-whu.github.io/DOTA/index.htmlhttp://captain.whu.edu.cn/DOTAweb/https://captain-whu.github.io/ODAI/https://arxiv.org/abs/1711.10398Dota is a large-scale dataset for object detection in aerial images. It can be used to develop and evaluate object detectors in aerial images. For the DOTA-v1.0, as described in the paper, it contains 2806 aerial images from different sensors and platforms. Each image is of the size in the range from about 800 × 800 to 4000 × 4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188, 282 instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral.Reference: Xia G S, Bai X, Ding J, et al. DOTA: A Large-scale Dataset for Object Detection in Aerial Images[J]. arXiv preprint arXiv:1711.10398, 2017. Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3974-3983 7. OpenSARShiphttp://opensar.sjtu.edu.cn/OpenSAR is an open SAR image management and processing platform developed by Advanced Sensing Technology Center (AST) of Shanghai Jiao Tong University for SAR image reading, processing, visualizing and algorithm testing. SAR image management and algorithm testing are the main tasks of OpenSAR. OpenSAR supports importing various SAR data source such as TerraSAR-X, RADARSAT 1/2, COSMO-SkyMed, etc. The users can search and view SAR image data by this platform. OpenSAR supports registering various algorithms such as image denoising, scene classification, target detection, target recognition, change detection, etc. The users can search, configure and execute these algorithms by this platform. A complete testing report will also be provided to the users.Reference: Huang L, Liu B, Li B, et al. OpenSARShip: A Dataset Dedicated to Sentinel-1 Ship Interpretation[J]. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2017. 8. ITCVDhttps://eostore.itc.utwente.nl:5001/fsdownload/zZYfgbB2X/ITCVDThe images were taken from an airplane platform which flied over Enschede, The Netherlands, in the height of ca 330m above the ground (Slagboom en Peeters, 2017). The images are taken in both nadir view and oblique view. The tilt angle of oblique view is 45 degrees. The Ground Sampling Distance (GSD) of the nadir images is 10cm. 135 images with 23543 vehicles are used for training and the remaining 38 images with 5545 vehicles for testing. Each vehicle in the dataset is manually annotated using a bounding box which is denoted as (x,y,w,h), where (x,y) is the coordinate of the left-up corner of the box, and (w,h) is the width and height of the box respectively.Reference: Yang M Y, Liao W, Li X, et al. Deep Learning for Vehicle Detection in Aerial Images[C]//2018 25th IEEE International Conference on Image Processing (ICIP). IEEE, 2018: 3079-3083. Yang M Y, Liao W, Li X, et al. Vehicle Detection in Aerial Images[J]. arXiv preprint arXiv:1801.07339, 2018. 9. DIUx xView 2018 Detection Challengehttp://xviewdataset.org/http://challenge.xviewdataset.orghttps://arxiv.org/abs/1802.07856 xView is one of the largest publicly available sets of overhead imagery. It contains images from complex scenes around the world, annotated with more than one million bounding boxes representing a diverse range of 60 object classes. Compared to other overhead imagery datasets, xView images are high-resolution, multi-spectral, and labeled with a greater variety of objects. Given a high-resolution satellite image, the Challenge task is to predict a bounding box for each object in the image. The DIUx xView Challenge is focused on accelerating progress in four computer vision frontiers: Reduce minimum resolution for detection; Improve learning efficiency; Enable discovery of more object classes; Improve detection of fine-grained classes. The DIUx xView Challenge follows in the footsteps of Challenges such as Common Objects in Context (COCO) and seeks to build off SpaceNet and Functional Map of the World (FMoW) to apply computer vision to the growing amount of available imagery from space so that we can understand the visual world in new ways and address a range of important applications.Reference: Lam D, Kuzma R, McGee K, et al. xView: Objects in Context in Overhead Imagery[J]. arXiv preprint arXiv:1802.07856, 2018. 10. Detecting Objects in Aerial Images (DOAI) (CVPR 2019 Workshop)https://captain-whu.github.io/DOAI2019/index.html Task1 - Detection with oriented bounding boxes Task2 - Detection with horizontal bounding boxes Task3 - Jointly object detection and orientation estimation for movable instances Dataset: DOTA-v1.5 contains 0.4 million annotated object instances within 19 categories, which is an updated version of DOTA-v1.0. Both of them use the same aerial images but DOTA-v1.5 has revised and updated the annotation of objects, where many small object instances about or below 10 pixels that were missed in DOTA-v1.0 have been additionally annotated. The categories of DOTA-v1.5 is also extended. Concretely, the categories of container crane, tractor are added. The large vehicle is split into bus and truck. The ship is split into large ship and small ship.In consistent with DOTA-v.1.0, the images in DOTA-v1.5 are mainly collected from the Google Earth, satellite JL-1, and satellite GF-2 of the China Centre for Resources Satellite Data and Application.The object categories in DOTA-v1.5 include: plane, large ship, small ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, bus, truck, small vehicle, helicopter, roundabout, soccer ball field, basketball court, container crane and tractor. Semantic Segmentation1. Zurich Summer Datasethttps://sites.google.com/site/michelevolpiresearch/data/zurich-dataset The “Zurich Summer v1.0” dataset is a collection of 20 chips (crops), taken from a QuickBird acquisition of the city of Zurich (Switzerland) in August 2002. QuickBird images are composed by 4 channels (NIR-R-G-B) and were pansharpened to the PAN resolution of about 0.62 cm GSD. We manually annotated 8 different urban and periurban classes : Roads, Buildings, Trees, Grass, Bare Soil, Water, Railways and Swimming pools. The cumulative number of class samples is highly unbalanced, to reflect real world situations. Note that annotations are not perfect, are not ultradense (not every pixel is annotated) and there might be some errors as well. We performed annotations by jointly selecting superpixels (SLIC) and drawing (freehand) over regions which we could confidently assign an object class.Reference: Volpi, M. &amp; Ferrari, V.; Semantic segmentation of urban scenes by learning local class interactions, In IEEE CVPR 2015 Workshop “Looking from above: when Earth observation meets vision” (EARTHVISION), Boston, USA, 2015. Volpi, M. &amp; Ferrari, V.; Structured prediction for urban scene semantic segmentation with geographic context, In Joint Urban Remote Sensing Event JURSE 2015, Lausanne, Switzerland, 2015. 2. ISPRS Test Project on Urban Classification and 3D Building Reconstruction—2D Semantic Labeling Contesthttp://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html To this end we provide two state-of-the-art airborne image datasets, consisting of very high resolution true ortho photo (TOP) tiles and corresponding digital surface models (DSMs) derived from dense image matching techniques. Both areas cover urban scenes. While Vaihingen is a relatively small village with many detached buildings and small multi story buildings, Potsdam shows a typical historic city with large building blocks, narrow streets and dense settlement structure. Each dataset has been classified manually into six most common land cover classes. We provide the classification data (label images) for approximately half of the images, while the ground truth of the remaining scenes will remain unreleased and stays with the benchmark test organizers to be used for evaluation of submitted results. Participants shall use all data with ground truth for training or internal evaluation of their method. Six categories/classes have been defined: Impervious surfaces (RGB: 255, 255, 255) Building (RGB: 0, 0, 255) Low vegetation (RGB: 0, 255, 255) Tree (RGB: 0, 255, 0) Car (RGB: 255, 255, 0) Clutter/background (RGB: 255, 0, 0) The clutter/background class includes water bodies (present in two images with part of a river) and other objects that look very different from everything else (e.g., containers, tennis courts, swimming pools) and that are usually not of interest in semantic object classification in urban scenes, however note that participants must submit labels for all classes (including the clutter/background class). For instance, it is not possible to submit only classification results for the category building. VaihingenThe data set contains 33 patches (of different sizes), each consisting of a true orthophoto (TOP) extracted from a larger TOP mosaic.The ground sampling distance of both, the TOP and the DSM, is 9 cm. The DSM was generated via dense image matching with Trimble INPHO 5.3 software and Trimble INPHO OrthoVista was used to generate the TOP mosaic. In order to avoid areas without data (“holes”) in the TOP and the DSM, the patches were selected from the central part of the TOP mosaic and none at the boundaries. Remaining (very small) holes in the TOP and the DSM were interpolated.The TOP are 8 bit TIFF files with three bands; the three RGB bands of the TIFF files correspond to the near infrared, red and green bands delivered by the camera.The DSM are TIFF files with one band; the grey levels (corresponding to the DSM heights) are encoded as 32 bit float values.The TOP and the DSM are defined on the same grid, so that it is not necessary to consider the geocoding information in the processing. PotsdamThe data set contains 38 patches (of the same size), each consisting of a true orthophoto (TOP) extracted from a larger TOP mosaicThe ground sampling distance of both, the TOP and the DSM, is 5 cm. The DSM was generated via dense image matching with Trimble INPHO 5.6 software and Trimble INPHO OrthoVista was used to generate the TOP mosaic. In order to avoid areas without data (“holes”) in the TOP and the DSM, the patches were selected from the central part of the TOP mosaic and none at the boundaries. Remaining (very small) holes in the TOP and the DSM were interpolated.The TOP come as TIFF files in different channel composistions, where each channel has a spectral resolution of 8bit: IRRG: 3 channels (IR-R-G) RGB: 3 channels (R-G-B) RGBIR: 4 channels (R-G-B-IR) In this way participants can pick the data needed conveniently.The DSM are TIFF files with one band; the grey levels (corresponding to the DSM heights) are encoded as 32 bit float values. The TOP and the DSM are defined on the same grid (UTM WGS84). Each tile comes with an affine transformation file (tiff world file) in order to enable a re-composition of images to larger mosaics if desired. In addition to the DSMs we provide so-called normalised DSMs, that is, after ground filtering the ground height is removed for each pixel, leading to an representation of heights above the terrain. This data was produced using some fully automatic filtering workflow, without manual quality control. Hence, we do not guarantee error free data here, this is just for researchers to help using height data, other than the absolute DSM. In the download folder you find the corresponding zip-file. If you unpack you find a readme.txt which you should read before using the data. The scripts based on lastools are also contained, so participants might want to tune them. We however do not provide support. 3. 2017 IEEE GRSS Data Fusion Contesthttp://www.grss-ieee.org/2017-ieee-grss-data-fusion-contest/http://dase.ticinumaerospace.com/index.phpThe 2017 Data Fusion Contest will consist in a classification benchmark. The task to perform is classification of land use (more precisely, Local Climate Zones, LCZ, Stewart and Oke, 2012) in various urban environments. Several cities have been selected to test the ability of LCZ prediction at generalizing all over the world. Input data are multi-temporal, multi-source and multi-modal (image and semantic layers). Local climate zones are a generic, climate-based typology of urban and natural landscapes, which delivers information on basic physical properties of an area that can be used by land use planners or climate modelers. LCZ are used as first order discretization of urban areas by the World Urban Database and Access Portal Tools initiative, which aims to collect, store and disseminate data on the form and function of cities around the world. The LCZ classes in this study correspond to those of [Stewart &amp; Oke, 2012]: 10 urban LCZs corresponding to various built types: Compact high-rise (class code in the ground truth: 1); Compact midrise (class code in the ground truth: 2); Compact low-rise (class code in the ground truth: 3); Open high-rise (class code in the ground truth: 4); Open midrise (class code in the ground truth: 5); Open low-rise (class code in the ground truth: 6); Lightweight low-rise (class code in the ground truth: 7); Large low-rise (class code in the ground truth: 8); Sparsely built (class code in the ground truth: 9); Heavy industry (class code in the ground truth: 10). 7 rural LCZs corresponding to various land cover types: Dense trees (class code in the ground truth: 11); Scattered trees (class code in the ground truth: 12); Bush and scrub (class code in the ground truth: 13); Low plants (class code in the ground truth: 14); Bare rock or paved (class code in the ground truth: 15); Bare soil or sand (class code in the ground truth: 16); Water (class code in the ground truth: 17). The contest aims to promote innovation in classification algorithms, as well as to provide objective and fair comparisons among methods. Ranking is based on quantitative accuracy parameters computed with respect to undisclosed test samples from cities unseen during training. Participants will be given a limited time to submit their classification maps after the competition is started. The contest will consist of two steps: Step 1 – training: Participants are provided with five training cities (Berlin, Rome, Paris, Sao Paulo, Hong Kong), including ground truth to train their algorithms. Step 2 – testing on new cities: Participants will receive the data of the test cities and will submit their classification maps by three weeks from the release of this second part of the data set. In parallel, they will submit a short description of the approach used. After evaluation of the results, 4 winners will be announced. The Data: Landsat data, in the form of images with 8 multispectral bands (i.e. visible, short and long infrared wavelengths) resampled at 100m resolution (courtesy of the U.S. Geological Survey); Sentinel2 images, with 9 multispectral bands (i.e. visible, vegetation red edges and short infrared wavelengths) resampled at 100m resolution (Contains modified Copernicus Data 2016); participants are encouraged to use the full resolution data, for which a direct link is provided in the data package. Ancillary data: Open Street Map (OSM) layers with land use information: building, natural, roads and land-use areas. We also provide rasterized versions of OSM layers at 20m resolution for building and land-use areas, superimposable with the satellite images. Moreover, for the training cities only, we also provide ground-truth of the various LCZ classes on several areas of the city (defined as polygons using the class codes above). They are provided as raster layers at 100m resolution, superimposable to the satellite images. The ground-truth for the test set will remain undisclosed and will be used for evaluation of the results. 4. Aerial Image Segmentation Datasethttps://zenodo.org/record/1154821#.XH6HtygzbIUGround truth of Berlin, Chicago, Paris, Potsdam, and Zurich consist of aerial images from Google Maps and pixel-wise building, road, and background labels from OpenStreetMap.Ground truth of Tokyo consists of one aerial image from Google Maps and manually generated, pixel-wise building, road, and background labels.Pixel-wise labels are provided as PNG images in RGB order. Pixels labeled as building, road, and background are indicated by RGB colors [255,0,0], [0,0,255], and [255,255,255]. RGB channel means of aerial images Berlin R: 79.94162, G: 84.72064, B: 78.94711 Chicago R: 86.46459, G: 85.73488, B: 77.14777 Paris R: 82.46727, G: 92.82243, B: 88.05664 Potsdam R: 74.85480, G: 77.37761, B: 70.22035 Tokyo R: 96.96883, G: 98.44344, B: 108.60135 Zurich R: 62.36962, G: 66.11001, B: 60.32863 Ground truth was generated in Berlin Spring 2016 Chicago Autumn 2015 Paris Autumn 2015 Potsdam Spring 2016 Tokyo Spring 2017 Zurich Autumn 2015 Reference: Kaiser P, Wegner J D, Lucchi A, et al. Learning aerial image segmentation from online maps[J]. IEEE Transactions on Geoscience and Remote Sensing, 2017, 55(11): 6054-6068. 5. 2018 IEEE GRSS Data Fusion Contest—Advanced multi-sensor optical remote sensing for urban land use and land cover classificationhttp://www.grss-ieee.org/community/technical-committees/data-fusion/2018-ieee-grss-data-fusion-contest/http://dase.ticinumaerospace.comThe 2018 IEEE GRSS Data Fusion Contest, organized by the Image Analysis and Data Fusion Technical Committee, aims to promote progress on fusion and analysis methodologies for multi-source remote sensing data. The 2018 Data Fusion Contest consists of a classification benchmark. The task to be performed is urban land use and land cover classification. To test their synergy as well as individual potential for urban land use and land cover classification, classification results can be submitted to three parallel and independent competitions: Data Fusion Classification Challenge (use of at least two data sets) Multispectral LiDAR Classification Challenge Hyperspectral Classification Challenge The Data:We provide (Acquired by the National Center for Airborne Laser Mapping, NCALM): The data were acquired by NCALM on February 16, 2017 between 16:31 and 18:18 GMT. Sensors used in this campaign include an Optech Titam MW (14SEN/CON340) with integrated camera (a LIDAR sensor operating at three different laser wavelengths), a DiMAC ULTRALIGHT+ (a very high resolution color imager) with a 70 mm focal length, and an ITRES CASI 1500 (a hyperspectral imager). Multispectral-LiDAR point cloud data at 1550 nm, 1064 nm, and 532 nm; Intensity rasters from first return per channel and DSMs at a 50-cm GSD. Hyperspectral data covering a 380-1050 nm spectral range with 48 bands at a 1-m GSD. Very high resolution RGB imagery at a 5-cm GSD. The image is organized into several separate tiles. The data were acquired by NCALM on February 16, 2017 between 16:31 and 18:18 GMT. Sensors used in this campaign include an Optech Titam MW (14SEN/CON340) with integrated camera (a LIDAR sensor operating at three different laser wavelengths), a DiMAC ULTRALIGHT+ (a very high-resolution color imager) with a 70mm focal length, and an ITRES CASI 1500 (a hyperspectral imager). The sensors were aboard a Piper PA-31- 350 Navajo Chieftain aircraft. Moreover, for the training region only, we also provide ground-truth corresponding to 20 urban land use and land cover classes. They are provided as raster at a 0.5-m GSD, superimposable to airborne images. The ground truth for the test set remains undisclosed and will be used for evaluation of the results. Urban Land Use and Land Cover Classes: 0 – Unclassified 1 – Healthy grass 2 – Stressed grass 3 – Artificial turf 4 – Evergreen trees 5 – Deciduous trees 6 – Bare earth 7 – Water 8 – Residential buildings 9 – Non-residential buildings 10 – Roads 11 – Sidewalks 12 – Crosswalks 13 – Major thoroughfares 14 – Highways 15 – Railways 16 – Paved parking lots 17 – Unpaved parking lots 18 – Cars 19 – Trains 20 – Stadium seats 6. EvLab-SS Datasethttp://earthvisionlab.whu.edu.cn/zm/SemanticSegmentation/index.htmlThe EvLab-SS benchmark is designed for the evaluation of the semantic segmentation algorithms on real engineered scenes, which aims to find a good deep learning architecture for the high resolution pixel-wise classification task in remote sensing area.The dataset is originally obtained from the Chinese Geographic Condition Survey and Mapping Project, and each image is fully annotated by the Geographic Conditions Survey (NO.GDPJ 01—2013) standards. The average resolution of the dataset is approximately 4500 × 4500 pixels. The EvLab-SS dataset contains 11 major classes, namely, background, farmland, garden, woodland, grassland, building, road, structures, digging pile, desert and waters, and currently includes 60 frames of images captured by different platforms and sensors. The dataset comprises 35 satellite images, 19 frames of which are captured by the World-View-2 satellite (re-sample GSD 0.2 m), 5 frames are captured by the GeoEye satellite (re-sample GSD 0.5 m), 5 frames are captured by the QuickBird satellite (re-sample GSD 2 m), 6 frames are captured by the GF-2 satellite (re-sample GSD 1 m). The dataset also has 25 aerial images, 10 images of which with spatial resolution of 0.25 m and 15 images have a spatial resolution of 0.1 m.Reference: Zhang M, Hu X, Zhao L, et al. Learning dual multi-scale manifold ranking for semantic segmentation of high-resolution images[J]. Remote Sensing, 2017, 9(5): 500. 7. DeepGlobe Land Cover Classification Challengehttp://deepglobe.org/index.htmlhttps://competitions.codalab.org/competitions/18468Automatic categorization and segmentation of land cover is of great importance for sustainable development, autonomous agriculture, and urban planning. We would like to introduce the challenge of automatic classification of land cover types. This problem is defined as a multi-class segmentation task to detect areas of urban, agriculture, rangeland, forest, water, barren, and unknown. The evaluation will be based on the accuracy of the class labels.Reference: Demir I, Koperski K, Lindenbaum D, et al. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images[J]. arXiv preprint arXiv:1805.06561, 2018. 8. Gaofen Image Dataset (GID)http://captain.whu.edu.cn/GID/Gaofen Image Dataset (GID) a large-scale dataset for land use and land cover (LULC) classification. It contains 150 high-quality Gaofen-2 (GF-2) images acquired from more than 60 different cities in China. And these images cover the geographic areas that exceed 50,000 km2. Images in GID have high intra-class diversity coupled with low inter-class separability. Therefore, GID can provide the research community with a high-quality data resource to advance the state-of-the-art in LULC classification.Reference: Tong X Y, Xia G S, Lu Q, et al. Learning Transferable Deep Models for Land-Use Classification with High-Resolution Remote Sensing Images[J]. arXiv preprint arXiv:1807.05713, 2018. 9. Airbus Ship Detection Challengehttps://www.kaggle.com/c/airbus-ship-detection 10. 2019 IEEE GRSS Data Fusion Contesthttp://www.grss-ieee.org/community/technical-committees/data-fusion/ Track 1: Single-view Semantic 3D Challenge Track 2: Pairwise Semantic Stereo Challenge Track 3: Multi-view Semantic Stereo Challenge Track 4: 3D Point Cloud Classification Challenge Reference: Bosch, M. ; Foster, G. ; Christie, G. ; Wang, S. ; Hager, G.D. ; Brown, M. : Semantic Stereo for Incidental Satellite Images. Proc. of Winter Conf. on Applications of Computer Vision, 2019. 11. 38-Cloud datasethttps://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset &emsp;&emsp;&emsp;Red &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Green &emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp; Blue &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; NIR &emsp;&emsp;&emsp;&emsp;&emsp; False color&emsp;&emsp;&emsp;Ground truth This dataset contains 38 Landsat 8 scene images and their manually extracted pixel-level ground truths for cloud detection. 38-Cloud dataset is introduced in [1], yet it is a modification of the dataset in [2].The entire images of these scenes are cropped into multiple 384384 patches to be proper for deep learning-based semantic segmentation algorithms. There are 8400 patches for training and 9201 patches for testing. Each patch has 4 corresponding spectral channels which are Red (band 4), Green (band 3), Blue (band 2), and Near Infrared (band 5). Unlike other computer vision images, these channels are not combined together. Instead, they are in their correspondig directories.*Reference: Mohajerani S, Krammer T A, Saeedi P. Cloud Detection Algorithm for Remote Sensing Images Using Fully Convolutional Neural Networks[J]. arXiv preprint arXiv:1810.05782, 2018. Mohajerani S, Saeedi P. Cloud-Net: An end-to-end Cloud Detection Algorithm for Landsat 8 Imagery[J]. arXiv preprint arXiv:1901.10077, 2019. 12. Slovenia 2017 Land Cover Classification Datasethttp://eo-learn.sentinel-hub.com/Land Cover Classification with eo-learn: Part 1Land Cover Classification with eo-learn: Part 2Land Cover Classification with eo-learn: Part 3http://eo-learn.sentinel-hub.com/eopatches_slovenia_2017_full.zip 187.0 GBeopatches_slovenia_2017_sample.zip 2.7 GBThe full dataset contains 293 EOPatches of the size of about 1000 x 1000 pixels at 10 m resolution, while the sample dataset contains 4 EOPatches from a 2x2 grid.Each EOPatch is a container of EO and non-EO data. 13. Aeroscapeshttps://github.com/ishann/aeroscapesThe AeroScapes aerial semantic segmentation benchmark comprises of images captured using a commercial drone from an altitude range of 5 to 50 metres. The dataset provides 3269 720p images and ground-truth masks for 11 classes.Reference: Nigam I, Huang C, Ramanan D. Ensemble knowledge transfer for semantic segmentation[C]//2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018: 1499-1508. Building Detection1. Massachusetts Buildings Datasethttps://www.cs.toronto.edu/~vmnih/data/ The size of all images in these datasets is 1500×1500, and the resolution is 1m2/pixel. The buildingdataset consists of 137 sets of aerial images and corresponding single-channel label images for training part, 10 for testing part, and 4 for validation part.Reference: Mnih V. Machine learning for aerial image labeling[D]. University of Toronto (Canada), 2013. 2. SpaceNet Buildings Datasethttps://spacenetchallenge.github.io/The Data - Over 685,000 footprints across the Five SpaceNet Areas of Interest. AOI Area of Raster (Sq. Km) Building Labels (Polygons) AOI_1_Rio 2,544 382,534 AOI_2_Vegas 216 151,367 AOI_3_Paris 1,030 23,816 AOI_4_Shanghai 1,000 92,015 AOI_5_Khartoum 765 35,503 SpaceNet Buildings Dataset Round 1CatalogThe data is hosted on AWS in a requester pays bucket.1aws s3 ls s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round1/ Training DataAOI 1 - Rio - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 1 (23 GB) with associated building footprints for training do the following:12aws cp s3://spacenet-dataset/spacenet_TrainData/3band.tar.gz .aws cp s3://spacenet-dataset/spacenet_TrainData/8band.tar.gz . Test DataAOI 1 - Rio - Building Extraction TestingTo download processed 200mx200m tiles of AOI 1 (7.9 GB) for testing do:12aws cp s3://spacenet-dataset/spacenet_TestData/3band.tar.gz .aws cp s3://spacenet-dataset/spacenet_TestData/8band.tar.gz . SpaceNet Buildings Dataset Round 2CatalogThe data is hosted on AWS in a requester pays bucket.1aws s3 ls s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/ Training Data (57.3 GB)1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/ . --recursive AOI 2 - Vegas - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 2 (23 GB) with associated building footprints for training do the following:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_2_Vegas_Train.tar.gz . AOI 3 - Paris - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 3 (5.3 GB) with associated building footprints do the following:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_3_Paris_Train.tar.gz . AOI 4 - Shanghai - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 4 (23.4 GB) with associated building footprints do the following:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_4_Shanghai_Train.tar.gz . AOI 5 - Khartoum - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 2 (4.7 GB) with associated building footprints do the following:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_5_Khartoum_Train.tar.gz . Test Data (19 GB)1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/ . --recursive AOI 2 - Vegas - Building Extraction TestingTo download processed 200mx200m tiles of AOI 2 (7.9 GB) for testing do:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_2_Vegas_Test_public.tar.gz . AOI 3 - Paris - Building Extraction TestingTo download processed 200mx200m tiles of AOI 3 (1.8 GB) for testing do:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_3_Paris_Test_public.tar.gz . AOI 4 - Shanghai - Building Extraction TestingTo download processed 200mx200m tiles of AOI 4 (7.7 GB) for testing do:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_4_Shanghai_Test_public.tar.gz . AOI 5 - Khartoum - Building Extraction TestingTo download processed 200mx200m tiles of AOI 2 (1.6 GB) for testing do:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_5_Khartoum_Test_public.tar.gz . 3. Aerial imagery object identification dataset for building and road detection, and building height estimationurl: https://figshare.com/collections/Aerial_imagery_object_identification_dataset_for_building_and_road_detection_and_building_height_estimation/3290519Automated object detection in high-resolution aerial imagery can provide valuable information in fields ranging from urban planning and operations to economic research, however, automating the process of analyzing aerial imagery requires training data for machine learning algorithm development. This dataset seeks to meet that need. For 25 locations across 9 U.S. cities, this dataset provides (1) high resolution aerial imagery; (2) annotations of over 40,000 building footprints (OSM shapefiles) as well as road polylines; and (3) topographical height data (LIDAR) . This dataset can be used as ground truth to train computer vision and machine learning algorithms for object identification and analysis, in particular for building detection and height estimation, as well as road detection.A complete description of the data can be found in the pdf file within the metadata for this collection, titled, “Metadata - building (area and height) and road dataset”. This data collection is organized such that there is a separate dataset in this collection for each of the 9 cities with available data (Arlington, MA; Atlanta, GA; Austin TX; Washington, DC; New Haven, CT; New York City, NY; Norfolk, VA; San Francisco, CA; Seekonk, MA). A map of these cities and example images can be found here: http://arcg.is/2afcSOk.Imagery data from the United States Geological Survey (USGS); building and road shapefiles are from OpenStreetMaps (OSM) (these OSM data are made available under the Open Database License: http://opendatacommons.org/licenses/odbl/1.0/); and the Lidar data are from U.S. National Oceanic and Atmospheric Administration (NOAA), the Texas Natural Resources Information System (TNRIS).Reference:1234567@misc&#123;bradbury_brigman_collins_johnson_lin_newell_park_suresh_wiesner_xi_2016, title=&#123;Aerial imagery object identification dataset for building and road detection, and building height estimation&#125;, DOI=&#123;10.6084/m9.figshare.c.3290519.v1&#125;, publisher=&#123;figshare&#125;, author=&#123;Bradbury, Kyle and Brigman, Benjamin and Collins, Leslie and Johnson, Timothy and Lin, Sebastian and Newell, Richard and Park, Sophia and Suresh, Sunith and Wiesner, Hoel and Xi, Yue&#125;, year=&#123;2016&#125;, month=&#123;Jul&#125;&#125; 4. Inria Aerial Image Labeling Datasethttps://project.inria.fr/aerialimagelabeling/ The Inria Aerial Image Labeling addresses a core topic in remote sensing: the automatic pixelwise labeling of aerial imagery (link to paper). Dataset features: Coverage of 810 km² (405 km² for training and 405 km² for testing)Aerial orthorectified color imagery with a spatial resolution of 0.3 mGround truth data for two semantic classes: building and not building (publicly disclosed only for the training subset)The images cover dissimilar urban settlements, ranging from densely populated areas (e.g., San Francisco’s financial district) to alpine towns (e.g,. Lienz in Austrian Tyrol). Instead of splitting adjacent portions of the same images into the training and test subsets, different cities are included in each of the subsets. For example, images over Chicago are included in the training set (and not on the test set) and images over San Francisco are included on the test set (and not on the training set). The ultimate goal of this dataset is to assess the generalization power of the techniques: while Chicago imagery may be used for training, the system should label aerial images over other regions, with varying illumination conditions, urban landscape and time of the year. Reference: Maggiori E, Tarabalka Y, Charpiat G, et al. Can semantic labeling methods generalize to any city? The INRIA aerial image labeling benchmark[C]//IEEE International Symposium on Geoscience and Remote Sensing (IGARSS). 2017. 5. The USSOCOM Urban 3D Challengehttps://www.topcoder.com/urban3dhttps://spacenetchallenge.github.io/datasets/Urban_3D_Challenge_summary.htmlhttps://github.com/topcoderinc/Urban3dThis challenge published a large-scale dataset containing 2D orthrorectified RGB and 3D Digital Surface Models and Digital Terrain Models generated from commercial satellite imagery covering over 360 km of terrain and containing roughly 157,000 annotated building footprints. All imagery products are provided at 50 cm ground sample distance (GSD). This unique 2D/3D large scale dataset provides researchers an opportunity to utilize machine learning techniques to further improve state of the art performance. Reliable labeling of buildings based on satellite imagery is one of the first and most challenging steps in producing accurate 3D models and maps. While automated algorithms continue to improve, significant manual effort is still necessary to ensure geospatial accuracy and acceptable quality. Improved automation is required to enable more rapid response to major world events such as humanitarian and disaster response. 3D height data can help improve automated building labeling performance, and capabilities for providing this data on a global scale are now emerging. In this challenge, we ask solvers to use satellite imagery and newly available 3D height data products to improve upon the state of the art for automated building detection and labeling. USSOCOM is seeking an algorithm that provides reliable, automatic labeling of buildings based solely on orthorectified color satellite imagery and 3D height data. Reference: H. Goldberg, M. Brown, and S. Wang, A Benchmark for Building Footprint Classification Using Orthorectified RGB Imagery and Digital Surface Models from Commercial Satellites, 46th Annual IEEE Applied Imagery Pattern Recognition Workshop, Washington, D.C, 2017. H. Goldberg, S. Wang, M. Brown, and G. Christie. Urban 3D Challenge: Building Footprint Detection Using Orthorectified Imagery and Digital Surface Models from Commercial Satellites. In Proceedings SPIE Defense and Commercial Sensing: Geospatial Informatics and Motion Imagery Analytics VIII, Orlando, Florida, USA, 2018. 6. DeepGlobe Building Extraction Challengehttp://deepglobe.org/index.htmlhttps://competitions.codalab.org/competitions/18544Modeling population dynamics is of great importance for disaster response and recovery, and detection of buildings and urban areas are key to achieve so. We would like to pose the challenge of automatically detecting buildings from satellite images. This problem is formulated as a binary segmentation problem to localize all building polygons in each area. The evaluation will be based on the overlap of detected polygons with the ground truth.Reference: Demir I, Koperski K, Lindenbaum D, et al. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images[J]. arXiv preprint arXiv:1805.06561, 2018. 7. CrowdAI Mapping Challengehttps://www.crowdai.org/challenges/mapping-challenge In this challenge you will be provided with a dataset of individual tiles of satellite imagery as RGB images, and their corresponding annotations of where an image is there a building. The goal is to train a model which given a new tile can annotate all buildings. ### Datasets - train.tar.gz : This is the Training Set of **280741** tiles (as **300x300** pixel RGB images) of satellite imagery, along with their corresponding annotations in MS-COCO format - val.tar.gz: This is the suggested Validation Set of **60317** tiles (as **300x300** pixel RGB images) of satellite imagery, along with their corresponding annotations in MS-COCO format - test_images.tar.gz : This is the Test Set for Round-1, where you are provided with **60697** files (as **300x300** pixel RGB images) and your are required to submit annotations for all these files. ## 8. WHU Building Dataset http://study.rsgis.whu.edu.cn/pages/download/ This dataset consists of an aerial dataset and a satellite dataset. 1、 **Aerial imagery dataset** The original aerial data comes from the New Zealand Land Information Services website. We manually edited Christchurch's building vector data, with about 22,000 independent buildings. The original ground resolution of the images is 0.075m. we provide manually edited shapefile corresponds to the whole area. We also down-sampled the most parts of aerial images (including 18,7000 buildings) to 0.3m ground resolution, and cropped them into 8,189 tiles with 512×512 pixels. The shapefile is also rasterized. The ready-to-use samples are divided into three parts: a training set (130,500 buildings), a validation set (14,500 buildings) and a test set (42,000 buildings). Training area:4736 tiles, Evaluation area: 1036 tiles, Test area: 2416 tiles ![](http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image001.jpg) 2、 **Satellite dataset I (global cities)** One of them is collected from cities over the world and from various remote sensing resources including QuickBird, Worldview series, IKONOS, ZY-3, etc. We manually delineated all the buildings. It contains 204 images (512 × 512 tiles with resolutions varying from 0.3 m to 2.5 m). Besides the differences in satellite sensors, the variations in atmospheric conditions, panchromatic and multispectral fusion algorithms, atmospheric and radiometric corrections and season made the samples suitable yet challenging for testing robustness of building extraction algorithms. ![](http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image002.jpg) 3、 **Satellite dataset Ⅱ (East Asia)** The other satellite building sub-dataset consists of 6 neighboring satellite images covering 550 km2 on East Asia with 2.7 m ground resolution. This test area is mainly designed to evaluate and to develop the generalization ability of a deep learning method on different data sources but with similar building styles in the same geographical area. The vector building map is also fully manually delineated in ArcGIS software and contains 29085 buildings. The whole image is seamlessly cropped into 17388 512×512 tiles for convenient training and testing with the same processing as in our aerial dataset. Among them 21556 buildings (13662 tiles) are separated for training and the rest 7529 buildings (3726 tiles) are used for testing. Training area: 13662 tiles Test area: 3726 tiles ![](http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image003.png) 4、 **Building change detection dataset** Our dataset covers an area where a 6.3-magnitude earthquake has occurred in February 2011 and rebuilt in the following years. This dataset consists of aerial images obtained in April 2012 that contains 12796 buildings in 20.5 km2 (16077 buildings in the same area in 2016 dataset). By manually selecting 30 GCPs on ground surface, the sub-dataset was geo-rectified to the aerial dataset with 1.6-pixel accuracy. This sub-dataset and the corresponding images from the original dataset are now openly provided along with building vector and raster maps. ![](http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image004.jpg) **Reference:** - *Ji S, Wei S, Lu M. Fully Convolutional Networks for Multisource Building Extraction From an Open Aerial and Satellite Imagery Data Set[J]. IEEE Transactions on Geoscience and Remote Sensing, 2018 (99): 1-13.* ## 9. AIRS (Aerial Imagery for Roof Segmentation) https://www.airs-dataset.com/ AIRS (Aerial Imagery for Roof Segmentation) is a public dataset that aims at benchmarking the algorithms of roof segmentation from very-high-resolution aerial imagery. The main features of AIRS can be summarized as: - 457km2 coverage of orthorectified aerial images with over 220,000 buildings - Very high spatial resolution of imagery (0.075m) - Refined ground truths that strictly align with roof outlines AIRS dataset covers almost the full area of Christchurch, the largest city in the South Island of New Zealand. The photography was taken during the flying seasons of 2015 and 2016, and the supplied images are ortho-rectified DOMs with RGB channels and 7.5cm resolution in projection of New Zealand Transverse Mercator. There are 226,342 labeled buildings within the whole area for experiment. To eliminate the impact of relief displacement, the ground truths for buildings are carefully refined to align with their roofs. Therefore, the segmentation task posed for AIRS contains two semantic classes: roof and non-roof pixels. **Reference:** - *Chen Q, Wang L, Wu Y, et al. Aerial Imagery for Roof Segmentation: A Large-Scale Dataset towards Automatic Mapping of Buildings[J]. arXiv preprint arXiv:1807.09532, 2018.* - *Chen Q, Wang L, Wu Y, et al. Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings[J]. ISPRS Journal of Photogrammetry and Remote Sensing, 2019, 147: 42-55.* ## 10. 2018 Open AI Tanzania Building Footprint Segmentation Challenge https://competitions.codalab.org/competitions/20100 ![](https://blog.werobotics.org/wp-content/uploads/2018/08/zmi-geonode.png) Data is provided in the form of GeoTIFF files along with GeoJSON files that contain the ground truth annotations. The "condition" property of each feature in the geojson file describes the condition of the building, any other fields in properties can be ignored. Training data can be obtained from the following Google Sheet: https://docs.google.com/spreadsheets/d/1tP133OqpwvkzHnkmS_3nezpTJMq06tpPM6P2qU6kaZ4/edit?usp=sharing ## 11. The SpaceNet Challenge Round 4 https://www.topcoder.com/spacenet https://community.topcoder.com/longcontest/?module=ViewProblemStatement&rd=17313&compid=71676 https://spacenetchallenge.github.io/datasets/spacenet-OffNadir-summary.html ### Objective Can you help us automate mapping from off-nadir imagery (collect from angles that are not straight down)? In this challenge, competitors are tasked with finding automated methods for extracting map-ready building footprints from high-resolution satellite imagery from high off-nadir imagery. In many disaster scenarios the first post-event imagery is from a more off-nadir image than is used in standard mapping use cases. Also, emerging next generation earth imaging satellite constellations that will provide more frequent revisists will tend to have higher-off nadir imagery. The ability to use higher off-nadir imagery will allow for more flexibility in acquiring and using satellite imagery after a disaster. Moving towards more accurate fully automated extraction of building footprints will help bring innovation to computer vision methodologies applied to high-resolution satellite imagery, and ultimately help create better maps where they are needed most. Your task will be to extract building footprints from increasingly off-nadir satellite images. The polygons you create will be compared to ground truth, and the quality of the solutions will be measured using the SpaceNet metric. ### Input Files This dataset contains 27 8-Band WorldView-2 images taken over Atlanta, GA on December 22nd, 2009. They range in off-nadir angle from 7 degrees to 54 degrees. For the competition, the 27 images are broken into 3 segments based on their off-nadir angle: - Nadir: 0-25 degrees - Off-nadir: 26 degrees - 40 degrees - Very Off-nadir 40-55 degrees The entire set of images is tiled into 450m x 450m tiles. The training data has 27 folders. “Atlanta_nadir{Off-Nadir Angle}_catid_{CatalogID}” Each Collection folder than has 3 types of images are available for the target areas: 1. PAN: panchromatic (single channel, 16-bit grayscale, ~50 cm resolution) 2. MUL: 8-band multi-channel (8*16-bit, ~2m resolution). This is the equivalent of the 8-band images from the first and second competition. 3. PanSharpen: pan-sharpened version of Red-Green-Blue+NIR1 bands from the multispectral product (4 channels, 3*16-bit, ~50 cm resolution). This is formed by using the PAN image to interpolate 4 bands of the MUL dataset to increase the resolution of the Red, Green and Blue bands. ## 12. built-structure-count dataset http://im.itu.edu.pk/deepcount/ https://arxiv.org/pdf/1904.00674.pdf # Road Detection ## 1. Massachusetts Roads Dataset https://www.cs.toronto.edu/~vmnih/data/ The size of all images in these datasets is **1500×1500**, and the resolution is **1m2/pixel**. The road dataset consists of **1108** sets for training part, **49** for testingpart, and **14** for validation part. **Reference:** - *Mnih V. Machine learning for aerial image labeling[D]. University of Toronto (Canada), 2013.* ## 2. SpaceNet Roads Dataset https://spacenetchallenge.github.io/ The Data - Over 8000 Km of roads across the four SpaceNet Areas of Interest. |AOI |Area of Raster (Sq. Km) |Road Centerlines (LineString)| |:-------------:|:-------------------------:|:---------------------------:| |AOI_2_Vegas |216 |3685 km| |AOI_3_Paris |1,030 |425 km| |AOI_4_Shanghai |1,000 |3537 km| |AOI_5_Khartoum |765 |1030 km| Road Type Breakdown (km of Road) |Road Type |AOI_2_Vegas |AOI_3_Paris |AOI_4_Shanghai |AOI_5_Khartoum |Total| |:---------:|:-------------:|:-------------:|:-------------:|:-------------:|:---:| |Motorway |115 |9 |102 |13 |240| |Primary |365 |14 |192 |98 |669| |Secondary |417 |58 |501 |66 |1042| |Tertiary |3 |11 |34 |68 |115| |Residential |1646 |232 |939 |485 |3301| |Unclassified |1138 |95 |1751 |165 |3149| |Cart track |2 |6 |19 |135 |162| |Total |3685 |425 |3537.9 |1030 |8677| **Catalog** The data is hosted on AWS in a requester pays bucket. 1aws s3 ls s3://spacenet-dataset/SpaceNet_Roads_Competition/ --request-payer requester **Sample Data** 10 Samples from each AOI - Road Network Extraction Samples To download processed 400mx400m tiles of AOI 2 (728.8 MB) with associated road centerlines for training do the following: 1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/SpaceNet_Roads_Sample.tar.gz --request-payer requester SpaceNet_Roads_Sample.tar.gz **Training Data** AOI 2 - Vegas - Road Network Extraction Training To download processed 400mx400m tiles of AOI 2 (25 GB) with associated road centerlines for training do the following: 1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_2_Vegas_Roads_Train.tar.gz --request-payer requester AOI_2_Vegas_Roads_Train.tar.gz AOI 3 - Paris - Road Network Extraction Training To download processed 400mx400m tiles of AOI 3 (5.6 GB) with associated road centerlines for training do the following: 1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_3_Paris_Roads_Train.tar.gz --request-payer requester AOI_3_Paris_Roads_Train.tar.gz AOI 4 - Shanghai - Road Network Extraction Training To download processed 400mx400m tiles of AOI 4 (25 GB) with associated road centerlines for training do the following: 1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_4_Shanghai_Roads_Train.tar.gz --request-payer requester AOI_4_Shanghai_Roads_Train.tar.gz AOI 5 - Khartoum - Road Network Extraction Training To download processed 400mx400m tiles of AOI 5 (25 GB) with associated road centerlines for training do the following: 1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_5_Khartoum_Roads_Train.tar.gz --request-payer requester AOI_5_Khartoum_Roads_Train.tar.gz **Test Data** AOI 2 - Vegas - Road Network Extraction Testing To download processed 400mx400m tiles of AOI 2 (8.1 GB) for testing do: 1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_2_Vegas_Roads_Test_Public.tar.gz --request-payer requester AOI_2_Vegas_Roads_Test_Public.tar.gz AOI 3 - Paris - Road Network Extraction Testing To download processed 400mx400m tiles of AOI 3 (1.9 GB) for testing do: 1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_3_Paris_Roads_Test_Public.tar.gz --request-payer requester AOI_3_Paris_Roads_Test_Public.tar.gz AOI 4 - Shanghai - Road Network Extraction Testing To download processed 400mx400m tiles of AOI 4 (8.1 GB) for testing do: 1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_4_Shanghai_Roads_Test_Public.tar.gz --request-payer requester AOI_4_Shanghai_Roads_Test_Public.tar.gz AOI 5 - Khartoum - Road Network Extraction Testing To download processed 400mx400m tiles of AOI 5 (8.1 GB) for testing do: 1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_5_Khartoum_Roads_Test_Public.tar.gz --request-payer requester AOI_5_Khartoum_Roads_Test_Public.tar.gz ## 3. DeepGlobe Road Extraction Challenge http://deepglobe.org/index.html https://competitions.codalab.org/competitions/18467 In disaster zones, especially in developing countries, maps and accessibility information are crucial for crisis response. We would like to pose the challenge of automatically extracting roads and street networks from satellite images. **Reference:** - *Demir I, Koperski K, Lindenbaum D, et al. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images[J]. arXiv preprint arXiv:1805.06561, 2018.* # Change Detection ## 1. Onera Satellite Change Detection https://rcdaudt.github.io/oscd/ The Onera Satellite Change Detection (OSCD) dataset address the issue of detecting changes between satellite images at different dates. It comprises **24 pairs of multispectral images** taken from the **Sentinel-2 satellite** in **2015** and **2018**. Locations are picked all over the world, in North and South America, Europe, Middle-East and Asia. For each location, registered pairs of 13-band multispectral satellite images obtained by the Sentinel-2 satellites are provided. Images vary in spatial resolution between 10m, 20m and 60m. Pixel-level change groundtruth is provided for **14** of the image pairs. The annotated changes focus on urban changes, such as new buildings or new roads. These data can be used for training and setting parameters of change detection algorithms. ![](https://www.onera.fr/sites/default/files/300/beirut-triptych.png) **References:** - *Urban Change Detection for Multispectral Earth Observation Using Convolutional Neural Networks R. Caye Daudt, B. Le Saux, A. Boulch, Y. Gousseau IEEE International Geoscience and Remote Sensing Symposium (IGARSS’2018) Valencia, Spain, July 2018* # Super Resolution ## PROBA-V Super Resolution https://kelvins.esa.int/proba-v-super-resolution/ In this competition you are given multiple images of each of 78 Earth locations and you are asked to develop an algorithm to fuse them together into a single one. The result will be a "super-resolved" image that is checked against a high resolution image taken from the same satellite, PROBA-V. The 'V' stands for Vegetation, which is the main focus of the on-board instruments. Can you enhance the vision of PROBA-V and help us advance the accuracy on monitoring earths vegetation growth? # 无人机数据 ## Vision Meets Drones: A Challenge (ECCV 2018 workshops) http://aiskyeye.com/ The VisDrone2018 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining , Tianjin University, China. The benchmark dataset consists of 263 video clips formed by 179,264 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). Note that, the dataset was collected using various drone platforms (i.e., drones with different models), in different scenarios, and under various weather and lighting conditions. These frames are manually annotated with more than 2.5 million bounding boxes of targets of frequent interests, such as pedestrians, cars, bicycles, and tricycles. Some important attributes including scene visibility, object class and occlusion, are also provided for better data utilization. four tasks: - (1) Task 1: object detection in images challenge. The task aims to detect objects of predefined categories (e.g., cars and pedestrians) from individual images taken from drones. - (2) Task 2: object detection in videos challenge. The task is similar to Task 1, except that objects are required to be detected from videos. - (3) Task 3: single-object tracking challenge. The task aims to estimate the state of a target, indicated in the first frame, in the subsequent video frames. - (4) Task 4: multi-object tracking challenge. The task aims to recover the trajectories of objects with (Task 4B) or without (Task 4A) the detection results in each video frame. **References:** - *Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Ling and Qinghua Hu, arXiv 2018. Vision Meets Drones: A Challenge.* ## The Highway Drone Dataset https://www.highd-dataset.com/ The highD dataset is a new dataset of naturalistic vehicle trajectories recorded on German highways. Using a drone, typical limitations of established traffic data collection methods such as occlusions are overcome by the aerial perspective. Traffic was recorded at six different locations and includes more than 110 000 vehicles. Each vehicle's trajectory, including vehicle type, size and manoeuvres, is automatically extracted. Using state-of-the-art computer vision algorithms, the positioning error is typically less than ten centimeters. Although the dataset was created for the safety validation of highly automated vehicles, it is also suitable for many other tasks such as the analysis of traffic patterns or the parameterization of driver models. **References:** - *Krajewski R, Bock J, Kloeker L, et al. The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving Systems[J]. arXiv preprint arXiv:1810.05642, 2018.* ## UAVid Dataset for Video Semantic Segmentation **References:** - *The UAVid Dataset for Video Semantic Segmentation[J]. arXiv preprint arXiv:1810.10438, 2018.* # Stereo Matching ## Middlebury http://vision.middlebury.edu/stereo/data/ ## KITTI http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow ## Sceneﬂow https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html ## ETH3D https://www.eth3d.net/ **References:** - *Schöps T, Schönberger J L, Galliani S, et al. A multi-view stereo benchmark with high-resolution images and multi-camera videos[C]//Conference on Computer Vision and Pattern Recognition (CVPR). 2017, 2017.* ## ISPRS ### 航空数据 一共提供了两组航空实验数据，一组是高楼林立的加拿大Toronto地区，另一组是半农村地区的德国Vaihingen地区。两组数据的航向重叠度为60%，旁向重叠度为30%。真值为LAS点云。 德国Vaihingen(斯图加特)地区有25张影像，4条航带 加拿大Toronto(多伦多)地区有13张影像，3条航带 ### 倾斜数据 http://www2.isprs.org/commissions/comm1/icwg15b/benchmark_main.html http://www2.isprs.org/commissions/comm1/icwg15b/benchmark/description_Dortmund.html http://www2.isprs.org/commissions/comm1/icwg15b/benchmark/Benchmark_Aim.html 两个地区Zurich(苏黎世) 和 Dortmund (Zeche Zollern)(多特蒙德) #### Zurich(苏黎世) 见EuroSDR航空影像密集匹配数据 #### Dortmund (Zeche Zollern)(多特蒙德) Dortmund (Zeche Zollern)(多特蒙德)无人机数据http://www.ifp.uni-stuttgart.de/ISPRS-EuroSDR/ImageMatching/index.en.htmlDortmund (Zeche Zollern)(多特蒙德)见红色区域 References: Nex，F.，Gerke，M.，Remondino，F.，Przybilla H.-J.，Bäumker，M.，Zurhorst，A.，2015。ISPRS Benchmark for Multi-Platform Photogrammetry。ISPRS Annals of the Photogrammetry，Remote Sensing and Spatial Information Sciences，Vol。II-3 / W4，pp.135-142. EuroSDR航空影像密集匹配数据http://www.ifp.uni-stuttgart.de/ISPRS-EuroSDR/ImageMatching/index.en.html试验数据包括两组垂直摄影和一组倾斜摄影。倾斜数据的覆盖区域是瑞士Zurich(苏黎世)，地面分辨率为6~13 cm，垂直影像的覆盖区域分别是德国Vaihingen(斯图加特)(地面分辨率为20 cm)和München(慕尼黑)(地面分辨率为10 cm)。References: Cavegn, S., Haala, N., Nebiker, S., Rothermel, M. &amp; Tutzauer, P., 2014. Benchmarking High Density Image Matching for Oblique Airborne Imagery. In: Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., Zürich, Switzerland, Vol. XL-3, pp. 45-52. IARPA Multi-View Stereo 3D Mapping Challengehttps://www.iarpa.gov/challenges/3dchallenge.htmlhttps://spacenetchallenge.github.io/datasets/mvs_summary.htmlhttp://www.jhuapl.edu/pubgeo/satellite-benchmark.htmlhttps://gfacciol.github.io/multi-date-stereo/This data set includes DigitalGlobe WorldView-3 panchromatic and multispectral images of a 100 square kilometer area near San Fernando, Argentina. We also provide 20cm airborne lidar ground truth data for a 20 square kilometer subset of this area and performance analysis software to assess accuracy and completeness metrics. Commercial satellite imagery is provided courtesy of DigitalGlobe, and ground truth lidar is provided courtesy of IARPA.Catalog1aws s3 ls s3://spacenet-dataset/mvs_dataset The catalog contains the following packages: Updated metric analysis software with examples from contest winners Challenge data package with instructions, cropped TIFF images, ground truth, image cropping software, and metric scoring software (1.2 GB) JHU/APL example MVS solution (451 MB) NITF panchromatic, multispectral, and short-wave infrared DigitalGlobe WorldView-3 satellite images (72.1 GB) LAZ lidar point clouds with SBET (2.2 GB) Spectral image calibration software (84 MB) Reference: M. Bosch, A. Leichtman, D. Chilcott, H. Goldberg, M. Brown. “Metric Evaluation Pipeline for 3D Modeling of Urban Scenes”, ISPRS Archives, 2017. M. Bosch, Z. Kurtz, S. Hagstrom, and M. Brown. “A multiple view stereo benchmark for satellite imagery”. In Proceedings of the Applied Imagery Pattern Recognition Workshop (AIPR), Washington, DC, USA, 2016 G. Facciolo, C. de Franchis, E. Meinhardt-Llopis, “Automatic 3D Reconstruction from Multi-Date Satellite Images,” IEEE International Conference on Computer Vision and Pattern Recognition, EARTHVISION Workshop, 2017. R. Qin, “Automated 3D recovery from very high resolution multi-view images,” ASPRS 2017 Annual Conference, 2017. Lidarhttp://www2.isprs.org/commissions/comm2/wg3/resources-and-links.html Oakland 3-D Point Cloud Datasethttp://www.cs.cmu.edu/~vmr/datasets/oakland_3d/cvpr09/doc/ Paris-rue-Madame datasethttp://www.cmm.mines-paristech.fr/~serna/rueMadameDataset.html IQmulus &amp; TerraMobilita Contesthttp://data.ign.fr/benchmarks/UrbanAnalysis/ District of Columbia - Classified Point Cloud LiDAR (Total Size: 279.8 GiB)https://registry.opendata.aws/dc-lidar-2015/ LiDAR point cloud data for Washington, DC is available for anyone to use on Amazon S3. This dataset, managed by the Office of the Chief Technology Officer (OCTO), through the direction of the District of Columbia GIS program, contains tiled point cloud data for the entire District along with associated metadata. Classified Point Cloud ClassesEach point within the point cloud has been classified according to the schema below. Class 1: Processed, but unclassified Class 2: Bare earth Class 7: Low noise Class 9: Water Class 10: Ignored ground Class 11: Withheld Class 17: Bridge decks Class 18: High noise Other datasetRSICDhttps://github.com/201528014227051/RSICD_optimalRSICD is used for remote sensing image captioning task. more than ten thousands remote sensing images are collected from Google Earth, Baidu Map, MapABC, Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images are 10921, with five sentences descriptions per image. To the best of our knowledge, this dataset is the largest dataset for remote sensing captioning. The sample images in the dataset are with high intra-class diversity and low inter-class dissimilarity. Thus, this dataset provides the researchers a data resource to advance the task of remote sensing captioning.Reference: Lu X, Wang B, Zheng X, et al. Exploring Models and Data for Remote Sensing Image Caption Generation[J]. IEEE Transactions on Geoscience and Remote Sensing, 2017. The IARPA Functional Map of the World (fMoW) Challengehttps://www.iarpa.gov/challenges/fmow.htmlhttps://spacenetchallenge.github.io/datasets/fmow_summary.htmlhttps://github.com/fmowIntelligence analysts, policy makers, and first responders around the world rely on geospatial land use data to inform crucial decisions about global defense and humanitarian activities. Historically, analysts have manually identified and classified geospatial information by comparing and analyzing satellite images, but that process is time consuming and insufficient to support disaster response. The fMoW Challenge sought to foster breakthroughs in the automated analysis of overhead imagery by harnessing the collective power of the global data science and machine learning communities; empowering stakeholders to bolster their capabilities through computer vision automation. The challenge published one of the largest publicly available satellite-image datasets to date, with more than one million points of interest from around the world. The dataset also contains other elements such as temporal views, multispectral imagery, and satellite-specific metadata that researchers can exploit to build novel algorithms capable of classifying facility, building, and land use.Reference: Gordon Christie, Neil Fendley, James Wilson, Ryan Mukherjee; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 6172-6180 IARPA CORE3D Public DataIARPA has publicly released DigitalGlobe satellite imagery for the Creation of Operationally Realistic 3D Environment (CORE3D) program to enable performer teams to crowdsource manual labeling efforts and to promote public research that aligns well with the CORE3D program’s objectives. The data is available for download using this torrent file and is now also available on SpaceNet. Catalog1aws s3 ls s3://spacenet-dataset/Hosted-Datasets/CORE3D-Public-Data/ The catalog contains the following packages: One WorldView-2 PAN and MSI image for Jacksonville, FL; Tampa, FL; Richmond, VA; and Omaha, NE Tiled WorldView-2 data sets including ground truth building labels for comparison with the USSOCOM Urban 3D Challenge 26 WorldView-3 PAN and MSI images over Jacksonville, FL 43 WorldView-3 PAN and MSI images over Omaha, NE 35 WorldView-3 PAN and MSI images over UCSD, CA 44 WorldView-2 PAN and MSI images over UCSD, CA Please consider referencing the following when reporting results using this data:Reference: M. Brown, H. Goldberg, K. Foster, A. Leichtman, S. Wang, S. Hagstrom, M. Bosch, and S. Almes, “Large-Scale Public Lidar and Satellite Image Data Set for Urban Semantic Labeling,” in Proc. SPIE Laser Radar Technology and Applications XXII, 2018. SEN1-2https://mediatum.ub.tum.de/1436631SEN1-2 is a dataset consisting of 282,384 pairs of corresponding synthetic aperture radar and optical image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, respectively. The SEN1-2 dataset is published to foster deep learning research in SAR-optical data fusion. Several possible applications, such as SAR image colorization, SAR-optical image matching, and creation of artificial optical images from SAR input data.Download： http://138.246.224.34/index.php/s/m1436631The data server also offers downloads with FTPThe data server also offers downloads with rsync (password m1436631):rsync rsync://m1436631@138.246.224.34/m1436631/ Okutama-Actionhttp://okutama-action.org/an aerial view concurrent human action detection dataset. It contains 43 fully-annotated sequences of 12 human action classes. They used two UAV pilots to capture 22 different scenarios using 9 participants from a distance of 10-45 meters and camera angles of 45-90. The dataset is in the form of 30 FPS videos that provides 77365 images. They splitted the dataset into the training (33 videos) and testing samples (10 videos). MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imageryhttp://mafatchallenge.mod.gov.il/https://competitions.codalab.org/competitions/19854https://groups.google.com/a/mafatchallenge.com/forum/?hl=en#!forum/mafat-challenge-forumhttps://grader.mafatchallenge.com/ DataThe dataset consists of aerial imagery taken from diverse geographical locations, different times, resolutions, area coverage and image acquisition conditions (weather, sun direction, camera direction, etc). Image resolution varies between 5cm to 15cm GSD (Ground Sample Distance). Few examples are presented below: Task SpecificationsParticipants are asked to classify objects in four granularity levels: Class - every object is categorized into one of the following major classes: ‘Large Vehicles’ or ‘Small Vehicles’. Subclass - objects are categorized to subclasses according to their function or designation, for example: Cement mixer, Crane truck, Prime mover, etc. Each object should be assigned to a single subclass. Presence of features - objects are labeled according to their characteristics. For example: has a Ladder? is Wrecked? has a Sunroof? etc. Each object may be labeled with multiple different features Object perceived color - Objects are labeled with their (human) percieved color. For example: Blue, Red, Yellow etc. Each object includes a single color value. Reference: Dahan E, Diskin T. COFGA: Classification Of Fine-Grained Features In Aerial Images[J]. arXiv preprint arXiv:1808.09001, 2018. MASATI dataset - MAritime SATellite Imagery datasethttp://www.iuii.ua.es/datasets/masati/This dataset provides maritime scenes of optical aerial images from visible spectrum. The MASATI dataset contains color images in dynamic marine environments, and it can be used to evaluate ship detection methods. Each image may contain one or multiple targets in different weather and illumination conditions. The datasets is composed of 6212 satellite images labeled according to the following seven classes: land, coast, sea, ship, multi, coast-ship, and detail. The satellite images were acquired from Bing Maps in RGB and with different sizes, as size is dependent on the region of interest to be registered in the image. In general, the average image size has a spatial resolution around 512 x 512 pixels. The images are stored as PNG where pixel values represent RGB colors. The distance between targets and the acquisition satellite has also been changed in order to obtain captures at different altitudes. Reference: Gallego A J, Pertusa A, Gil P. Automatic Ship Classification from Optical Aerial Images with Convolutional Neural Networks[J]. Remote Sensing, 2018, 10(4): 511.]]></content>
      <categories>
        <category>Dataset</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - Context Encoding for Semantic Segmentation(CVPR2018)]]></title>
    <url>%2F2018%2F06%2F11%2FContext-Encoding-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[提出了Context Encoding Module来捕获场景的语义上下文并选择性地强调与类别相关的特征图，所提出的EncNet实现了新的state-of-the-art的结果。在PASCAL-Context达到51.7% mIoU, 在PASCAL VOC 2012达到85.9% mIoU，单个模型在ADE20K test set达到0.5567,超过了COCO-Place Challenge 2017的冠军。Context Encoding Module也可以改善用于分类的相对较浅的网络的特征表达，在CIFAR-10上14层的网络实现了3.45%的错误率与state-of-the-art approaches的10倍多层网络的精度相当。 paper: https://arxiv.org/abs/1803.08904http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Context_Encoding_for_CVPR_2018_paper.htmlcode: https://github.com/zhanghang1989/PyTorch-Encodingauthor：Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal 1. Introduction最近语义分割网络使用基于多分辨率金字塔来扩大感受野，如PSPSNet中的Spatial Pyramid Pooling和Deeplab中的Atrous Spatial Pyramid Pooling。虽然这些方法可以提升性能，但是上下文地表达不明确，导致了一个问题：增加感受野就等同于整合了上下文信息了吗？如图所示是来自于ADE20K中的一张图片，包含150个类别。如果允许标注者先选择图像的语境(如卧室)，接着工具会提供一个小的相关的类别列表(如床，椅子等)，这会极大地减少可能的类别搜索空间。同样，如果可以设计一个方法来充分利用场景语境类别的概率的关系，对网络来说语义分割会更容易。是否可以把传统方法的上下文编码和深度学习结合？之前作者就设计了一个Encoding Layer整合字典学习和残差编码过程到一个CNN层中来捕获无序的表达，在纹理分类中获得了state-of-the-art的结果。在这篇论文中作者延申Encoding Layer来捕获全局特征统计信息来理解语义上下文。这篇论文的两个贡献： 提出了Context Encoding Module。在Context Encoding Module中还包含Semantic Encoding Loss(SE-Loss)，一个利用全局场景上下文信息的单元。Context Encoding Module可以捕获场景的语义上下文并选择性地强调与类别相关的特征图。常规的训练过程中只用了逐像素的分割loss，没有利用场景的上下文。引入Semantic Encoding Loss(SE-Loss)来正则化训练，让网络预测在场景中物体种类的出现概率来强制网络学习语义上下文。而且SE-Loss同等对待大物体和小物体，发现小物体的实际分割效果有提升。 设计了一个语义分割网络Context Encoding Network (EncNet)。EncNet在预训练的ResNet中通过引入了Context Encoding Module强化网络。使用了dilation convolution。 2. Context Encoding Module Context EncodingEncoding Layer输入为$H×W×C$的特征图，之后reshape为$N(H×W)$个$C$维的vector $X=\lbrace{x_1,…,x_N}\rbrace$，在Encoding Layer中要学习codebook $D=\lbrace{d_1,…,d_K}\rbrace$其中包含K个codeword以及smoothing factor $S=\lbrace{s_1,…,s_K}\rbrace$。Encoding Layer输出为k个残差编码$e_k$，每个残差编码的维度为$C$，残差编码$e_k$是对soft-assignment weights的残差求和得到的: $e_k=\sum_{i=1}^N{e_{ik}}$。残差计算方法：$r_{ik}=x_i-d_k$soft-assignment weights残差计算方法：$e_{ik}=\frac{exp(-s_k{\left|r_{ik}\right|}^2)}{\sum_{j=1}^K{exp(-s_j{\left|r_{ij}\right|}^2)}}r_{ik}$。最终对编码求和而不是concatenate，即$e=\sum_{k=1}^K{\phi{(e_k)}}$，其中$\phi$为BN和ReLU，这样避免了K个编码有序同时也减少了特征表达的维度。 Featuremap Attention为了利用编码层捕获的编码的语义信息，预测了特征图的权重因子最为反馈来强调或不强调与类别相关的特征图。这里使用了全连接层和sigmoid激活函数使得输出为0到1之间。然后与特征图作逐通道相乘，与SENet类似。 Semantic Encoding Loss由于标准的训练过程中网络从孤立的像素计算loss，，因此网络没有全局信息很难理解上下文，为了正则化Context Encoding Module的训练过程，引入了Semantic Encoding Loss(SE-loss)，SE-loss以非常小的计算代价强制网络理解全局信息。在Encoding Layer上额外加入一个全连接层和sigmoid激活函数用binary cross entropy loss来预测在场景中是否有某一类存在。在实际中，发现对于小物体的分割有改进。 2.1 Context Encoding Network (EncNet)在Context Encoding Module基础上用预训练的ResNet构造了Context Encoding Network (EncNet)。在EncNet的stage3和stage4中也采用了dilated network strategy使输出为原图大小的1/8，之后上采样8倍还原。在ResNet最后加入Context Encoding Module和SE-loss。因为Context Encoding Module和SE-loss是轻量级的，所以在stage3后也加入了Context Encoding Module和SE-loss。 例如特征图大小为$32×32×512$，reshape后得到1024(32*32)个512维的向量，在Encoding Layer中有k=32个512维的向量$d_k$，残差编码$e_1$是1024个向量与32个向量中的第一个向量差的和，其他残差编码同理。这样得到32个残差编码，之后通过BN和ReLU在求和得到512维的向量再通过fc层和sigmoid激活函数得到每个通道的权重。作者这里用了residual，将加权后的特征图与原特征图求和后通过conv6使通道数与类别数一致最后再上采样与原始图像大小一致。在selayer中用fc层得到num_classes个输出来计算SE-loss。在stage3阶段的另外一个SE-loss的计算是先将特征图通过一个3×3的卷积层使得通道数由1024变为256，再通过一个1×1的卷积层使得通道数与类别数一致，最后再上采样与原始图像大小一致。 code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class EncNet(BaseNet): def __init__(self, nclass, backbone, aux=True, se_loss=True, norm_layer=nn.BatchNorm2d, **kwargs): super(EncNet, self).__init__(nclass, backbone, aux, se_loss, norm_layer=norm_layer) self.head = EncHead(self.nclass, in_channels=2048, se_loss=se_loss, norm_layer=norm_layer, up_kwargs=self._up_kwargs) if aux: self.auxlayer = FCNHead(1024, nclass, norm_layer=norm_layer) def forward(self, x): imsize = x.size()[2:] #features = self.base_forward(x) _, _, c3, c4 = self.base_forward(x) x = list(self.head(c4)) x[0] = upsample(x[0], imsize, **self._up_kwargs) if self.aux: auxout = self.auxlayer(c3) auxout = upsample(auxout, imsize, **self._up_kwargs) x.append(auxout) return tuple(x)class EncModule(nn.Module): def __init__(self, in_channels, nclass, ncodes=32, se_loss=True, norm_layer=None): super(EncModule, self).__init__() if isinstance(norm_layer, encoding.nn.BatchNorm2d): norm_layer = encoding.nn.BatchNorm1d else: norm_layer = nn.BatchNorm1d self.se_loss = se_loss self.encoding = nn.Sequential( encoding.nn.Encoding(D=in_channels, K=ncodes), norm_layer(ncodes), nn.ReLU(inplace=True), encoding.nn.Sum(dim=1)) self.fc = nn.Sequential( nn.Linear(in_channels, in_channels), nn.Sigmoid()) if self.se_loss: self.selayer = nn.Linear(in_channels, nclass) def forward(self, x): en = self.encoding(x) b, c, _, _ = x.size() gamma = self.fc(en) y = gamma.view(b, c, 1, 1) # residual ? outputs = [x + x * y] if self.se_loss: outputs.append(self.selayer(en)) return tuple(outputs)class EncHead(nn.Module): def __init__(self, out_channels, in_channels, se_loss=True, norm_layer=None, up_kwargs=None): super(EncHead, self).__init__() self.conv5 = nn.Sequential( nn.Conv2d(in_channels, 512, 3, padding=1, bias=False), norm_layer(512), nn.ReLU(True)) self.encmodule = EncModule(512, out_channels, ncodes=32, se_loss=se_loss, norm_layer=norm_layer) self.dropout = nn.Dropout2d(0.1, False) self.conv6 = nn.Conv2d(512, out_channels, 1) self.se_loss = se_loss def forward(self, x): x = self.conv5(x) outs = list(self.encmodule(x)) outs[0] = self.conv6(self.dropout(outs[0])) return tuple(outs) 打印出来网络结构如下12345678910111213141516171819202122232425262728293031(head): EncHead( (conv5): Sequential( (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace) ) (encmodule): EncModule( (encoding): Sequential( (0): Encoding(N x 512=&gt;32x512) (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace) (3): Sum() ) (fc): Sequential( (0): Linear(in_features=512, out_features=512, bias=True) (1): Sigmoid() ) (selayer): Linear(in_features=512, out_features=59, bias=True) ) (dropout): Dropout2d(p=0.1) (conv6): Conv2d(512, 59, kernel_size=(1, 1), stride=(1, 1)) )(auxlayer): FCNHead( (conv5): Sequential( (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Dropout2d(p=0.1) (4): Conv2d(256, 59, kernel_size=(1, 1), stride=(1, 1)) ) 3. Experimental Results3.1 Implementation DetailsADE20K数据集初始学习率0.01另外两个数据集0.001。momentum 为 0.9weight decay 为 0.000150 epochs on PASCAL Context and PASCAL VOC 2012, and 120 epochs on ADE20K数据增强： randomly shufﬂe the training samples and discard the last mini-batch randomly ﬂip scale the image between 0.5 to 2 randomly rotate the image between -10 to 10 degree crop the image into ﬁx size using zero padding if needed 训练时为了增大batchsize，使用了Synchronized Cross-GPU Batch Normalization，使用了mini-batch size为16，Encoding Layer中的codeword设为32，ground truth通过unique操作来找到在ground truth中出现的类别。最终损失函数为per-pixel segmentation loss和SE-loss的加权求和。 3.2 Results on PASCAL-ContextAblation Study测试了SE-loss的不同权重$\alpha=\lbrace{0.0,0.1,0.2,0.4,0.8}\rbrace$，发现0.2的效果最好。测试了Encoding Layer中codeword K的取值，最后使用K=32。更深的基础网络的精度提供更好的精度，用ResNet101最为基础网络，mIoU提升2.5%。EncNet比之前的state-of-the-art方法更好，并且没有使用COCO预训练或使用更深的ResNet152。 3.3 Results on PASCAL VOC 2012 3.4 Results on ADE20K 3.4 Image Classiﬁcation Results on CIFAR-10]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Channel Attention mechanism</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation]]></title>
    <url>%2F2018%2F06%2F10%2FESPNet-Efficient-Spatial-Pyramid-of-Dilated-Convolutions-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[作者提出了efﬁcient spatial pyramid(ESP) module构建了一个轻量级的语义分割网络。 project page: https://sacmehta.github.io/ESPNet/paper: https://arxiv.org/abs/1803.06815code: https://github.com/sacmehta/ESPNet ESP module在ESP module中将一个标准卷积分解为2步：point-wise convolution 和 spatial pyramid of dilated convolution。point-wise convolution：特征图通道维度由M变为$d=\frac{N}{K}$spatial pyramid of dilated convolution：分为K个分支，每个分支的dilation为$2^{K-1}$的卷积,最后concatenate在一起.ESP module的参数量为$\frac{MN}{K}+\frac{(nN)^2}{K}$，感受野为$[(n-1)2^{K-1}+1]^2$。而标准卷积的参数量为$n^2NM$ Hierarchical feature fusion (HFF)为了解决由于引入dilated convolution带来的网格效应，将不同dilation的特征图分层求和，然后再concatenate。 与其他网络建立策略的比较 ESP module： reduce-split-transform-merge MobileNet module：depth-wise convolutions (transform) and point-wise convolutions (expand)。当K=N时，除了卷积操作的顺序不同外，和MobileNet module一样。 ShufﬂeNet module：reduce-transform-expand。将原来ResNet中的bottleneck block中1×1的卷积和3×3的卷积替换为1×1的grouped convolution和3×3的depth-wise convolution。 Inception module：f split-reduce-transform-merge ResNext module：split-reduce-transform-expand-merge Atrous spatial pyramid (ASP) module：split-transform-merge Network structure]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Real-Time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - RTSeg: Real-time Semantic Segmentation Comparative Study(Accepted in IEEE ICIP 2018)论文阅读 - ShuffleSeg：Real-time Semantic Segmentation Network]]></title>
    <url>%2F2018%2F06%2F10%2FRTSeg-Real-time-Semantic-Segmentation-Comparative-Study%2F</url>
    <content type="text"><![CDATA[在第一篇论文中，作者将语义分割网络分为特征提取阶段和解码阶段，比较了了不同种特征提取模块和解码模块的组合来达到实时语义分割。 特征提取模块：VGG16，ResNet18，MobileNet，ShuffleNet解码模块：SkipNet，UNet，Dilation Frontend 在第二篇论文中，作者受ShuffleNet启发，提出了ShuffleSeg语义分割网络，在encoding阶段利用了group convolution和channel shufﬂing，比较了不同的decoding方法，发现UNet可以达到最好的精度，skip architecture可以在精度和实时性取得较好的权衡。 Paper: https://arxiv.org/abs/1803.02758 https://arxiv.org/abs/1803.03816Code: https://github.com/MSiam/TFSegmentation RTSeg: Real-time Semantic Segmentation Comparative Study ShuffleSeg：Real-time Semantic Segmentation Network]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Real-Time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - DeepLab V3+——Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation]]></title>
    <url>%2F2018%2F06%2F03%2FEncoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation%2F</url>
    <content type="text"><![CDATA[在DeepLab V3+中通过采用了encoder-decoder结构，在DeepLab V3中加入了一个简单有效的decoder模块来改善物体边缘的分割结果。除此之外还尝试使用Xception作为encoder，在Atrous Spatial Pyramid Pooling和decoder中应用depth-wise separable convolution得到了更快精度更高的网络，在PASCAL VOC 2012数据集上达到state-of-art的效果。 Paper: https://arxiv.org/abs/1802.02611 Code: https://github.com/tensorflow/models/tree/master/research/deeplab 1. Introduction在Introduction和related works中，作者回顾语义分割中的常用的两种结构： 空间金字塔池化 encoder-decoder空间金字塔池化(图a)可以池化不同分辨率的特征图来捕获丰富的上下文信息，而encoder-decoder结构(图b)则可以获得锋利的边界。因此，在DeepLab V3+中通过采用了encoder-decoder结构，在DeepLab V3中加入了一个简单有效的decoder模块来改善物体边缘的分割结果(图c)：先上采样4倍，在与encoder中的特征图concatenate，最后在上采样4倍恢复到原始图像大小。除此之外还尝试使用Xception作为encoder，在Atrous Spatial Pyramid Pooling和decoder中应用depth-wise separable convolution得到了更快精度更高的网络。 2. Methods encoderencoder就是DeepLab V3，通过修改ResNet101最后两(一)个block的stride，使得output stride为8(16)。之后在block4后应用改进后的Atrous Spatial Pyramid Pooling，将所得的特征图concatenate用1×1的卷积得到256个通道的特征图。 decoder在decoder中，特征图首先上采样4倍，然后与encoder中对应分辨率低级特征concatenate。在concatenate之前，由于低级特征图的通道数通常太多(256或512)，而从encoder中得到的富含语义信息的特征图通道数只有256，这样会淡化语义信息，因此在concatenate之前，需要将低级特征图通过1×1的卷积减少通道数。在concatenate之后用3×3的卷积改善特征，最后上采样4倍恢复到原始图像大小。 将Xception作为encoder原始的Xception结构如下：采用的Xception模型为MSRA team提出的改进的Xception，叫做Aligned Xception，并做了几点修改： 网络深度与Aligned Xception相同，不同的地方在于不修改entry flow network的结构，为了快速计算和有效的使用内存。 所有的max pooling操作替换成带stride的separable convolution，这能使得对任意分辨率的图像应用atrous separable convolution提取特征。 在每个3×3的depath-wise convolution后增加BN层和ReLU。 3. Experimental Evaluationlearning rate schedule：polyinitial learning rate 0.007crop size 513×513 3.1. Decoder Design Choicesdecoder的设计主要考虑三点： 1×1卷积的通道数。最后采用了48。 用来获得更锋利的边界的3×3的卷积。最后采用了2个3×3的卷积。在卷积类型方面，实验发现2个3×3的卷积效果要比1个或者3个3×3的卷积，或者卷积核为1×1的效果好。 所使用的encoder的低级特征。当同时使用conv2和conv3的特征时，先上采样2倍与conv3的特征concatenate，然后再上采样2倍与conv2的特征concatenate。这种结构并没有观察到显著的改进。因此最后只利用了conv2的特征。 3.2. ResNet-101 as Network Backbone 3.3. Xception as Network Backbone首先在ImageNet上预训练Xception。 官方执行结果link：https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md 3.4. Improvement along Object Boundaries使用trimap实验测量模型在分割边界的准确度。计算边界周围扩展频带(称为trimap)内的mIoU。结果如下：附MobileNet V2应用DeepLab V3+的结果]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Dilation Convolution</tag>
        <tag>encoder-decoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - Stacked Deconvolutional Network for Semantic Segmentation]]></title>
    <url>%2F2018%2F06%2F01%2FStacked-Deconvolutional-Network-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[作者是中科院自动化所CASIA_IVA_JD团队，在COCO Places Challenges 2017 Scene Parsing取得了冠军，其中就运用了这个网络。提出了Stacked Deconvolutional Network(SDN)，网络的第一个encoder为DenseNet161，后续SDN通过堆叠多个浅层的Deconvolutional Network来整合上下文信息和恢复位置信息，为了帮助网络训练和特征重用加入了inter-unit连接和intra-unit连接，同时用hierarchical supervision使网络各个部分学习到有判别力发的特征表达，有利于网络优化。 Paper: https://arxiv.org/abs/1708.04943Slide：http://presentations.cocodataset.org/Places17-CASIA_IVA_JD.pdf 比赛中采用了SDN，改进后的DeepLabV3和ResNet38三个模型集成。]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>encoder-decoder</tag>
        <tag>deconvolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - Gated Feedback Refinement Network for Dense Image Labeling]]></title>
    <url>%2F2018%2F05%2F31%2FGated-Feedback-Refinement-Network-for-Dense-Image-Labeling%2F</url>
    <content type="text"><![CDATA[在本文中作者在encoder-decoder结构的Skip Connection中利用门控机制结合深层特征减少浅层特征得歧义性。 Paper: http://openaccess.thecvf.com/content_cvpr_2017/html/Islam_Gated_Feedback_Refinement_CVPR_2017_paper.html Code: https://github.com/mrochan/gfrnet Gated Feedback Reﬁnement Network作者认为在encoder-decoder结构中直接利用Skip Connection将encoder中的特征传到decoder中是有问题的，因为encoder中的特征相对较浅，具有一定的歧义性，更深层的特征由于有较大的感受野更具有判别能力(如上图)，因此为了减少这种歧义性，利用gating mechanism将浅层特征和深层特征结合再通过Skip Connection传到decoder中。作者所提出的Gated Feedback Reﬁnement Network(G-FRNet)如图所示，encoder网络基于VGG-16，移除了最后的softmax层和全连接层，在其后又加了两个卷积层conv6和conv7，这样对于一幅输入图像就获得了7个大小的特征图($f_1,f_2,…,f_7$)。decoder网络为作者提出的Feedback Reﬁnement Network(FRN)，在decoder中通过使用gating mechanism调节由Skip Connection传来的信息。通过对$f_7$应用通道数类别数3×3的卷积得到第一幅粗略的预测图$Pm^G$。之后的预测图以$Pm^{RU_1}$为例，原始的Skip Connection将$f_5$与对$Pm^G$卷积之后的特征图进行concatenate。而作者首先基于$f_5$和更深层的特征图$f_6$通过gate unit得到gated特征图$G_1$(这么做的原因就是上面说的)来过滤掉类别的歧义性。然后与粗略的预测结果$Pm^G$结合得到大分辨率的特征图$Pm^{RU_1}$。重复这个过程得到后续的预测图$Pm^{RU_2}$，$Pm^{RU_3}$，$Pm^{RU_4}$，$Pm^{RU_5}$。 其中gate unit如图所示，分别对$f^i_g$和$f^{i+1}_g$做3×3的卷积使得$f^{i+1}_g$的通道数与$f^i_g$相同。然后上采样两倍得到$f^{i+1}_{g^{‘}}$，这样特征图大小也相同，最后两个特征图逐元素相乘得到$M_f$。 网络中的RU代表Gated Reﬁnement Unit，其结构如图所示。Gated Reﬁnement Unit的输入为粗略的预测图$R_f$和gated特征图$M_f$。首先对gated特征图$M_f$做3×3的卷积得到特征图$m_f$，然后$m_f$与$R_f$concatenate，最后再做一次3×3的卷积得到特征图$R’_f$。上采样两倍后传入下一个Gated Reﬁnement Unit。可视化结果如下 网络最后会得到6个特征图$Pm^G$，$Pm^{RU_1}$，$Pm^{RU_2}$，$Pm^{RU_3}$，$Pm^{RU_4}$，$Pm^{RU_5}$，每个特征图都可以计算loss，最终的loss fuction由这6个loss求和得到。 Experiments实验平台：caffeGPU：Titan x网络conv1到conv5用VGG-16初始化，其他卷积层Xavier初始化输入图像大小: Pascal VOC 320×320 CamVid 360×480]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>encoder-decoder</tag>
        <tag>gating mechanism</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - DeepLab V3--Rethinking Atrous Convolution for Semantic Image Segmentation]]></title>
    <url>%2F2018%2F05%2F30%2FDeepLab-V3-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation%2F</url>
    <content type="text"><![CDATA[在DeepLab的第3个版本中，作者主要通过串联或并行Dilation Convolution解决多尺度的问题，并且优化了第2版中提出的Atrous Spatial Pyramid Pooling module，在PASCAL VOC 2012数据集上达到state-of-art的效果。 Paper: https://arxiv.org/abs/1706.05587 Code: https://github.com/tensorflow/models/tree/master/research/deeplab 1. Introduction在这篇论文中，作者提出语义分割的挑战有2个(在上一个版本的论文中作者认为有3个，第三个是由于DXNN的invariance而带来的定位精度的减少，当时的解决方案是CRF)： 由于连续的pooling操作和stride不为1的卷积所造成的特征图的大小不断减小，但是这样却可以让DCNN学到更抽象的特征表达，解决方案是使用dilated Convolution。 另一个挑战是由于物体的多尺度问题。 作者分析了现有的解决多尺度问题的解决方法： 输入多个尺度的图像，分别训练，最后融合预测结果 encoder-decoder结构利用来自于encoder部分的多尺度特征来恢复decoder部分的空间分辨率 在网络后增加额外的模块来捕获大范围的信息，如DenseCRF 采用Spatial Pyramid Pooling 作者采用了串联或者并行的Atrous Spatial Pyramid Pooling module来克服多尺度的问题，设计了DeepLab V3。 2. Related Work在这里作者主要讨论了4种类型的FCN。 Image pyramid这种网络用小尺度的输入来编码大范围的上下文信息，而大尺度的输入用来保留小物体的细节。这种网络的缺点为：对于很大或者很深的网络由于GPU内存的限制并不能很好的同时输入多个尺度的图像进行训练，因此它经常用于测试阶段。 Encoder-decoder这种网络由两部分组成：在encoder中随着特征图大小的逐渐减少，越容易捕获大范围的信息，在decoder中逐渐地恢复特征图的大小。如SegNet，UNet，RefineNet等。 Context module这种网络包含额外的模块来编码大范围的上下文信息。例如DenseCRF. Spatial pyramid pooling这种模型应用金字塔池化在几个范围内来捕获上下文信息。如ParseNet，DeepLab V2，PSPNet。 在本文中，作者则用dilated convolution作为上下文模块和工具进行空间金字塔池化。 3. Methods作者首先以串联方式设计artous convolution模块。在ResNet的最后一个模块叫做block4，在block4后复制block4 3次得到block5，block6，block7。每个block中都包含3个3×3的卷积，除了在最后一个block stride为1，剩余blockstride都为2。如图中(a)所示。这种连续stride使得更深的block可以捕获大范围的信息，但是对于语义分割来说是有害的，因为细节都丢失了。所以在block3开始用artous convolution，rate依次为2，4，8，16。 3.1 Multi-grid MethodMulti-grid Method是指在block4到block7中采用不同的atrous rate。这是通过串联dilation convolution解决多尺度的问题。 3.2 Atrous Spatial Pyramid Pooling作者发现当在3×3的卷积采用越来越大的atrous rates的时候，有效的滤波权重越来越小，在极端情况下，退化成一个简单的1×1的卷积。因此为了克服这个问题且整合全局上下文信息，作者采用了图像级特征，即采用全局平均值池化得到1×1的特征图，然后双线性插值到需要的分辨率。最后改进版的ASPP由两部分组成：(a) 1个1×1的卷积和3个3×3的rate为(6,12,18)的卷积(全部都有batch normalization)(b) 全局平均值池化 所有分支之后concatenate传到另一个1×1的卷积(有batch normalization)，最后传到最终的1×1的卷积。 4. Experimental Evaluation实验设置：学习率策略：polycrop size：513上采样预测结果：在之前地工作中，训练时都是将ground truth下采样，但是作者发现下采样ground truth会移除一些细小地标注从而导致细节没有反向传播，因此保持ground truth不动非常重要，因此训练时采用上采样最终特征图。数据增强：随即缩放，随机左右翻转。 以串联方式应用artous convolution首先对于不同地output stride进行了比较，可以看到随着特征图地不断减小，导致语义分割地结果会持续下降：在保持output stride为16时，在RenNet50和ResNet101中添加不同数量串联block地比较在保持output stride为16时，在ResNet101中添加不同数量串联block和多种rate地artous卷积地比较在训练时output stride为16，但是在测试时output stride变为8可以得到更好地精度，当采用了多尺度地输入和翻转输入时，精度进一步提升。 以并行方式应用artous convolution，即Atrous Spatial Pyramid Pooling]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Dilation Convolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - Dilated Residual Networks]]></title>
    <url>%2F2018%2F05%2F30%2FDilated-Residual-Networks%2F</url>
    <content type="text"><![CDATA[将Dilation Convolution引入Image Classification任务中，使得在最终的特征图不至于太小而丢失太多的细节。为了由于引入Dilation Convolution带来的网格效应，提出了移除网格效应的方法。除此之外DRN还可以用于object localization和semantic segmentation任务中。 Paper: https://arxiv.org/abs/1705.09914 http://openaccess.thecvf.com/content_cvpr_2017/html/Yu_Dilated_Residual_Networks_CVPR_2017_paper.html Code: https://github.com/fyu/drn 1. Dilated Residual Networks如何将ResNet转变为DRN呢？ 将group 4的第一层stride变为1，dilation变为1，group 4中剩余卷积层和group 5的第一层dilation变为2，group 5中剩余卷积层dilation变为4。 2. Localization 如何将DRN用于Localization呢？ 直接移除global average pooling层，之后用1×1的卷积激活函数为softmax生成n个通道特征图。每一层中的像素值对应每类物体出现的概率。这样没有增加参数，也不需要重新训练模型用于Localization，可以直接将模型用于Localization。 3. Degridding由于引入Dilation Convolution带来的网格效应，作者通过3个步骤逐渐移除网格效应。 Removing max pooling。作者发现网络中的max pooling会带来高频的激活，这种高频的激活会传导到后面的层，最后加剧网格效应。因此取代max pooling层为卷积层。具体为：第一层7×7的卷积核数量由64变为16，然后跟随两个residual block。下图显示了这一操作的影响：DRN-A-18为max pooling之后的特征图，DRN-B-26为将max pooling变为卷积层后的特征图。 Adding layers。在网络的最后逐渐减少dilation，增加了dilation为2的residual block和dilation为1的block。这样ResNet-18由于前两步的操作增加了6层，变为DRN-B-26。 Removing residual connections。由于在上一步中最后加入的residual block含有residual connections，这仍然会带来网格效应，因此需要将level 7和level 8的residual connections移除，这就是最后的DRN-C-26。实验表明：DRN-C-26具有和DRN-A-34相似的精度，比DRN-A-50还高的定位精度和语义分割精度。 在code中作者还设计了DRN-C的简化版DRN-D，DRN-D在7×7的卷积之后去掉了2个3×3的卷积，也去掉了最后2个3×3的卷积。减少了4层 4. Experiments4.1 Image Classification 4.2 Weakly-supervised Object Localization 4.3 Semantic Segmentation]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Dilation Convolution</tag>
        <tag>Image Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - Pixel Deconvolutional Networks]]></title>
    <url>%2F2018%2F05%2F30%2FPixel-Deconvolutional-Networks%2F</url>
    <content type="text"><![CDATA[作者为了解决deconvolution中出现的棋盘格效应，提出了pixel deconvolutional layer(PixelDCL)，来直接建立上采样特征图相邻像素的关系，可以直接替换掉任何deconvolution layer。 Paper: https://arxiv.org/abs/1705.06820 Code: https://github.com/divelab/PixelDCN 棋盘格效应： 效果图： 第三行为使用deconvolution的结果，第四行为作者提出的PixelDCL的结果。 1. Deconvolution layer1维的deconvolution：输入特征图为4×1，输出特征图为8×1。输入特征图的每个像素与卷积核相乘，结果依次偏移两个值，结果特征图的像素值为每列求和。可以看到紫色像素只和(1,3)有关，橙色像素只和(2,4)有关。因此可以分解为两个独立的卷积，如右图所示。可以看出相邻像素没有直接的关系，这就导致了棋盘格效应。 2维的deconvolution：对于2维的deconvolution，同理输入特征图为4×4，输出特征图为8×8，中间由不同的卷积核得到4个特征图，最终输出特征图由4个中间特征图重新排列得到。同样相邻像素没有直接的关系，这就导致了棋盘格效应。 2. Pixel deconvolutional layer在作者提出的PixelDCL中，中间特征图图是依次生成的，只有第一个中间特征图和输入特征图有关，后续所有中间特征图都只和之前的特征图有关。最终输出特征图还是由4个中间特征图重新排列得到。实际上，作者还提出了一种input pixel deconvolutional layer (iPixelDCL)，中间特征图不仅和之前的特征图有关，还和输入特征图有关，但是实验发现PixelDCL的效果比iPixelDCL和deconvolution效果好。 PixelDCL可以应用于语义分割网络，VAE和GAN中，作者将PixelDCL应用于UNet和VAE中测试了PixelDCL的效果。实际中，作者设计了一个更加简单的PixelDCL。如下图所示： 3. 实验结果]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>deconvolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读 - ICNet for Real-Time Semantic Segmentation on High-Resolution Images(ECCV 2018 CUHK, SenseTime)]]></title>
    <url>%2F2018%2F05%2F29%2FICNet-for-Real-Time-Semantic-Segmentation-on-High-Resolution-Images%2F</url>
    <content type="text"><![CDATA[Uses deep supervision and runs the input image at different scales, each scale through their own subnetwork and progressively combining the results project：https://hszhao.github.io/projects/icnet/ Paper: https://arxiv.org/abs/1704.08545 Code: https://github.com/hszhao/ICNet 1.Introduction当前fast semantic segmentation的状态下图是在Cityscapes数据集上测试速度和mIoU的比较： 一方面，可以看到大多数现有方法几乎都不能达到实时的要求。另一方面，虽然SegNet，ENet，SQ速度较快，但是mIoU低于60%。因此作者提出建立一个实用的快速语义分割框架。核心思想是：让低分辨率的图像先通过网络得到粗略的预测图，然后所提出的cascade fusion unit引入中等分辨率和高分辨率的图像逐渐改善预测图。 主要贡献 提出了image cascade network (ICNet)，它可以同时有效地利用低分辨率地语义信息和高分辨率的细节。 所提出的ICNet实现了5倍多的加速，减少了5倍多的内存消耗。 可以在1024×2048的图像上达到30.3fps的同时实现高质量的结果。 2. Related Work分别对High Quality Semantic Segmentation，Fast Semantic Segmentation和Video Segmentation Architectures进行回顾。 3. Speed Analysis3.1. Time Budget在这里作者先回顾了PSPNet的分割性能，然后引入了加速语义分割的直观策略，从它们网络的去缺点中，描述所提出的image cascade框架和cascade feature fusion unit。下图展示了PSPNet50(这里是优化后的，详细的变化在原文第6节)在512×1024和1024×2048分辨率下的运行时间。很显然当图像变大时，运行时间必然增加。同时网络的宽度(或卷积核的数量)也会影响运行时间。如在stage4 和 stage5阶段有相同的空间分辨率，但是在stage5的时间是stage4的4倍多。这是因为在stage5中的卷积核的数量是stage4的两倍。 3.2. Intuitive Speedup这里主要有三点： 1.输入降采样后的图像具体做法是：输入1/2或1/4的降采样后的图像，得到预测结果后，再上采样到原始大小。这样做的缺点是虽然时间减少了但是预测的结果非常粗糙，丢失了许多小的但是却重要的细节。如下图： 2.除了直接降采样图像外，另一个直接的方法是降采样特征图。在FCN中降采样32倍，DeepLab中降采样了8倍。作者测试PSPNet50降采样8倍，16倍，32倍的结果如下： 可以看到小的特征图需要的时间更少，而且即使是最小的特征图仍然需要131ms，达不到实时的要求。 3.除了以上策略外，另一个自然的方式是模型压缩。作者测试了最近提出的一种压缩方法，发现并没有满足实时的要求。即使只保留1/4的卷积核，所需要的时间还是太长了，与此同时mIuO非常低，不能产生合理的分割图。 4. Image Cascade Network4.1 主要结构和分支 首先将输入图像缩放到原图的1/2和1/4，分别将3个尺度的图像输入网络的3个分支。 对于低分辨率的图像，即原始图像的1/4大小，通过卷积及池化后缩小了1/8，对应原图的1/32，然后应用dilated convlution来增加感受野，输出为原始图像大小的1/32。 对于中等分辨率的图像，即原始图像的1/2大小，同样通过几个卷积层和池化层(和上面一个分支共享一部分卷积层来减少参数数量)后缩小了8倍，即原始图像的1/16。为了融合1/32的特征图和1/8的特征图，作者提出了cascade feature fusion(CFF)单元来产生1/16的特征图。 对于高等分辨率的图像，即原始图像，同样通过几个卷积层和池化层后缩小了8倍，得到1/8的特征图。为了融合1/32的特征图和1/8的特征图，作者提出了cascade feature fusion(CFF)单元来产生1/16的特征图。由于在中等分辨率时，已经恢复了大多数在低分辨率中丢失的语义信息，因此可以限制处理高分辨率时的卷积层的数量。只使用了3×3的卷积核和stride为2来降采样到原图的1/8。然后使用CFF单元整合由中等分辨率得到的特征图和原始图片得到的特征图。最终得到原始图像的1/8的特征图。 Cascade Label Guidance为了辅助学习过程，作者提出了Cascade Label Guidance策略：在训练时每个分支上采样2倍后，同时也将ground truth降采样1/16，1/8和1/4(紫线)，这样损失函数就有3项。在测试时将低分辨率和中等分辨率的这一操作丢弃。Cascade Label Guidance策略可以保证训练迭代更容易，梯度优化更平滑。 4.2 分支分析在ICnet中在第二分支有17个卷积层，第三分支只有3个卷积层，而且第一分支和第二分支共享部分计算。最深的网络结构应用在低分辨率图像上，这可以有效的提取大多数语义信息。即使超过50层，测试时运行时间不超过18ms，内存消耗不超过0.6G。因此，所提出的ICNet是一个非常高效和节省内存的结构。 4.3 和其他Cascade Structures的不同之处其他Cascade Structures都聚焦于融合单尺度或多尺度的输入的不同层的特征，而ICNet则用低分辨率的图像通过主体语义分割分支，再用高分辨率信息进行优化。 5. Cascade Feature Fusion and Final Model 为了结合不同分辨率的图像，提出了Cascade Feature Fusion(CCF)单元，输入包含3个部分：两个特征图$F_1$和$F_2$，大小为$H_1\times{W_1\times{C_1}}$和$H_2\times{W_2\times{C_2}}$,和一个ground truth label大小为$H_2\times{W_2\times{1}}$。$F_2$的大小是$F_1$的2倍。首先将$F_1$上采样2倍使得与$F_2$大小相同，然后用$3\times3$的卷积核，dilation 1的dilated convolution对上采样的特征图refine。而对于特征图$F_2$用$1\times1$的卷积使得与特征图$F_1$具有相同的通道数。在dilated convolution和$1\times1$的卷积后都使用了Batch normalization。最后两个特征图相加再通过ReLU激活函数，得到融合的特征图F^{‘}_2。除此之外，还对对上采样的特征图$F_1$添加了辅助分类器，辅助分类器损失的权重设为0.4。 5.1 The Loss Function最终的损失函数由3部分组成： L=\lambda_{1}L_1+\lambda_{2}L_2+\lambda_{3}L_35.2 Final Model Compression最后为了进一步减少运行时间，对模型进行压缩。作者采取的是渐进式的方式。以压缩率为1/2为例，并没有直接移除一半的卷积核，而是先保存3/4的卷积核再进行fine-tuning。之后再移除更多的卷积核，再fine-tuning直到达到1/2的压缩率。 如何选择哪个卷积核要被移除呢？ 作者通过计算卷积核的L1范数，再根据L1范数进行排序，去掉那些较小的卷积核。这种方式不会剧烈的更新所有参数，因此可以较好地减少网络大小。 6. Experimental Evaluation实验平台：caffe CUDA7.5 CUDNN V5 GPU：TitanX 基础网络：PSPNet 修改： 在金字塔池化模块时的concat操作变为sum，减少特征维度从4096到2048 改变金字塔池化后的卷积操作的卷积核大小由$3\times3$变为$1\times1$ batch size：16 base learning rate：0.01 learning policy：poly max iteration：30k momentum：0.9 weight decy：0.0001 数据增强：随机镜像和随机缩放到0.5到2倍之间。 6.1 Model Compression以PSPNet50为baseline使用相同的模型压缩方法，与ICNet比较。 6.2 Ablation Study for Image Cascade Framework Video example：]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Real-Time</tag>
      </tags>
  </entry>
</search>
